digraph {
	graph [size="156.75,156.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140403847059072 [label="
 ()" fillcolor=darkolivegreen1]
	140403824589264 [label=MeanBackward0]
	140403824590800 -> 140403824589264
	140403824590800 [label=AddmmBackward0]
	140403824590320 -> 140403824590800
	140403847057872 [label="module.linear.bias
 (10)" fillcolor=lightblue]
	140403847057872 -> 140403824590320
	140403824590320 [label=AccumulateGrad]
	140403824590368 -> 140403824590800
	140403824590368 [label=ViewBackward0]
	140403864602800 -> 140403824590368
	140403864602800 [label=AvgPool2DBackward0]
	140403864601312 -> 140403864602800
	140403864601312 [label=ReluBackward0]
	140403864602464 -> 140403864601312
	140403864602464 [label=CudnnBatchNormBackward0]
	140403864602368 -> 140403864602464
	140403864602368 [label=ConvolutionBackward0]
	140403824394400 -> 140403864602368
	140403824394400 [label=AddBackward0]
	140403824394544 -> 140403824394400
	140403824394544 [label=CudnnBatchNormBackward0]
	140403824394688 -> 140403824394544
	140403824394688 [label=ConvolutionBackward0]
	140403824394880 -> 140403824394688
	140403824394880 [label=ReluBackward0]
	140403824395024 -> 140403824394880
	140403824395024 [label=CudnnBatchNormBackward0]
	140403824395072 -> 140403824395024
	140403824395072 [label=ConvolutionBackward0]
	140403824395360 -> 140403824395072
	140403824395360 [label=ReluBackward0]
	140403824395504 -> 140403824395360
	140403824395504 [label=CudnnBatchNormBackward0]
	140403824395552 -> 140403824395504
	140403824395552 [label=ConvolutionBackward0]
	140403824395840 -> 140403824395552
	140403824395840 [label=AddBackward0]
	140403824395984 -> 140403824395840
	140403824395984 [label=CudnnBatchNormBackward0]
	140403824396128 -> 140403824395984
	140403824396128 [label=ConvolutionBackward0]
	140403824396320 -> 140403824396128
	140403824396320 [label=ReluBackward0]
	140403824396464 -> 140403824396320
	140403824396464 [label=CudnnBatchNormBackward0]
	140403824396512 -> 140403824396464
	140403824396512 [label=ConvolutionBackward0]
	140403824396800 -> 140403824396512
	140403824396800 [label=ReluBackward0]
	140403824396944 -> 140403824396800
	140403824396944 [label=CudnnBatchNormBackward0]
	140403824396992 -> 140403824396944
	140403824396992 [label=ConvolutionBackward0]
	140403824395936 -> 140403824396992
	140403824395936 [label=AddBackward0]
	140403824397376 -> 140403824395936
	140403824397376 [label=CudnnBatchNormBackward0]
	140403824397520 -> 140403824397376
	140403824397520 [label=ConvolutionBackward0]
	140403824397712 -> 140403824397520
	140403824397712 [label=ReluBackward0]
	140403824397856 -> 140403824397712
	140403824397856 [label=CudnnBatchNormBackward0]
	140403824397904 -> 140403824397856
	140403824397904 [label=ConvolutionBackward0]
	140403824398192 -> 140403824397904
	140403824398192 [label=ReluBackward0]
	140403824398288 -> 140403824398192
	140403824398288 [label=CudnnBatchNormBackward0]
	140403824398448 -> 140403824398288
	140403824398448 [label=ConvolutionBackward0]
	140403824397328 -> 140403824398448
	140403824397328 [label=CudnnBatchNormBackward0]
	140403824398832 -> 140403824397328
	140403824398832 [label=ConvolutionBackward0]
	140403824399024 -> 140403824398832
	140403824399024 [label=ReluBackward0]
	140403824399168 -> 140403824399024
	140403824399168 [label=CudnnBatchNormBackward0]
	140403824399216 -> 140403824399168
	140403824399216 [label=ConvolutionBackward0]
	140403824399504 -> 140403824399216
	140403824399504 [label=ReluBackward0]
	140403824399648 -> 140403824399504
	140403824399648 [label=CudnnBatchNormBackward0]
	140403824399696 -> 140403824399648
	140403824399696 [label=ConvolutionBackward0]
	140403824399984 -> 140403824399696
	140403824399984 [label=AddBackward0]
	140403824400128 -> 140403824399984
	140403824400128 [label=CudnnBatchNormBackward0]
	140403824400272 -> 140403824400128
	140403824400272 [label=ConvolutionBackward0]
	140403824400464 -> 140403824400272
	140403824400464 [label=ReluBackward0]
	140403824400608 -> 140403824400464
	140403824400608 [label=CudnnBatchNormBackward0]
	140403824400656 -> 140403824400608
	140403824400656 [label=ConvolutionBackward0]
	140403824400944 -> 140403824400656
	140403824400944 [label=ReluBackward0]
	140403824401088 -> 140403824400944
	140403824401088 [label=CudnnBatchNormBackward0]
	140403824401136 -> 140403824401088
	140403824401136 [label=ConvolutionBackward0]
	140403824400080 -> 140403824401136
	140403824400080 [label=AddBackward0]
	140403824401520 -> 140403824400080
	140403824401520 [label=CudnnBatchNormBackward0]
	140403824401664 -> 140403824401520
	140403824401664 [label=ConvolutionBackward0]
	140403824401856 -> 140403824401664
	140403824401856 [label=ReluBackward0]
	140403824402000 -> 140403824401856
	140403824402000 [label=CudnnBatchNormBackward0]
	140403824402048 -> 140403824402000
	140403824402048 [label=ConvolutionBackward0]
	140403824402336 -> 140403824402048
	140403824402336 [label=ReluBackward0]
	140403824402384 -> 140403824402336
	140403824402384 [label=CudnnBatchNormBackward0]
	140403824378016 -> 140403824402384
	140403824378016 [label=ConvolutionBackward0]
	140403824401472 -> 140403824378016
	140403824401472 [label=AddBackward0]
	140403824378400 -> 140403824401472
	140403824378400 [label=CudnnBatchNormBackward0]
	140403824378544 -> 140403824378400
	140403824378544 [label=ConvolutionBackward0]
	140403824378736 -> 140403824378544
	140403824378736 [label=ReluBackward0]
	140403824378880 -> 140403824378736
	140403824378880 [label=CudnnBatchNormBackward0]
	140403824378928 -> 140403824378880
	140403824378928 [label=ConvolutionBackward0]
	140403824379216 -> 140403824378928
	140403824379216 [label=ReluBackward0]
	140403824379360 -> 140403824379216
	140403824379360 [label=CudnnBatchNormBackward0]
	140403824379408 -> 140403824379360
	140403824379408 [label=ConvolutionBackward0]
	140403824379696 -> 140403824379408
	140403824379696 [label=AddBackward0]
	140403824379840 -> 140403824379696
	140403824379840 [label=CudnnBatchNormBackward0]
	140403824379984 -> 140403824379840
	140403824379984 [label=ConvolutionBackward0]
	140403824380176 -> 140403824379984
	140403824380176 [label=ReluBackward0]
	140403824380320 -> 140403824380176
	140403824380320 [label=CudnnBatchNormBackward0]
	140403824380368 -> 140403824380320
	140403824380368 [label=ConvolutionBackward0]
	140403824380656 -> 140403824380368
	140403824380656 [label=ReluBackward0]
	140403824380800 -> 140403824380656
	140403824380800 [label=CudnnBatchNormBackward0]
	140403824380848 -> 140403824380800
	140403824380848 [label=ConvolutionBackward0]
	140403824379792 -> 140403824380848
	140403824379792 [label=AddBackward0]
	140403824381232 -> 140403824379792
	140403824381232 [label=CudnnBatchNormBackward0]
	140403824381376 -> 140403824381232
	140403824381376 [label=ConvolutionBackward0]
	140403824381568 -> 140403824381376
	140403824381568 [label=ReluBackward0]
	140403824381712 -> 140403824381568
	140403824381712 [label=CudnnBatchNormBackward0]
	140403824381760 -> 140403824381712
	140403824381760 [label=ConvolutionBackward0]
	140403824365728 -> 140403824381760
	140403824365728 [label=ReluBackward0]
	140403824365872 -> 140403824365728
	140403824365872 [label=CudnnBatchNormBackward0]
	140403824365920 -> 140403824365872
	140403824365920 [label=ConvolutionBackward0]
	140403824381184 -> 140403824365920
	140403824381184 [label=AddBackward0]
	140403824366304 -> 140403824381184
	140403824366304 [label=CudnnBatchNormBackward0]
	140403824366448 -> 140403824366304
	140403824366448 [label=ConvolutionBackward0]
	140403824366640 -> 140403824366448
	140403824366640 [label=ReluBackward0]
	140403824366784 -> 140403824366640
	140403824366784 [label=CudnnBatchNormBackward0]
	140403824366832 -> 140403824366784
	140403824366832 [label=ConvolutionBackward0]
	140403824367120 -> 140403824366832
	140403824367120 [label=ReluBackward0]
	140403824367264 -> 140403824367120
	140403824367264 [label=CudnnBatchNormBackward0]
	140403824367312 -> 140403824367264
	140403824367312 [label=ConvolutionBackward0]
	140403824366256 -> 140403824367312
	140403824366256 [label=CudnnBatchNormBackward0]
	140403824367696 -> 140403824366256
	140403824367696 [label=ConvolutionBackward0]
	140403824367888 -> 140403824367696
	140403824367888 [label=ReluBackward0]
	140403824368032 -> 140403824367888
	140403824368032 [label=CudnnBatchNormBackward0]
	140403824368080 -> 140403824368032
	140403824368080 [label=ConvolutionBackward0]
	140403824368368 -> 140403824368080
	140403824368368 [label=ReluBackward0]
	140403824368512 -> 140403824368368
	140403824368512 [label=CudnnBatchNormBackward0]
	140403824368560 -> 140403824368512
	140403824368560 [label=ConvolutionBackward0]
	140403824368848 -> 140403824368560
	140403824368848 [label=AddBackward0]
	140403824368992 -> 140403824368848
	140403824368992 [label=CudnnBatchNormBackward0]
	140403824369136 -> 140403824368992
	140403824369136 [label=ConvolutionBackward0]
	140403824369328 -> 140403824369136
	140403824369328 [label=ReluBackward0]
	140403824369472 -> 140403824369328
	140403824369472 [label=CudnnBatchNormBackward0]
	140403824369520 -> 140403824369472
	140403824369520 [label=ConvolutionBackward0]
	140403824349392 -> 140403824369520
	140403824349392 [label=ReluBackward0]
	140403824349536 -> 140403824349392
	140403824349536 [label=CudnnBatchNormBackward0]
	140403824349584 -> 140403824349536
	140403824349584 [label=ConvolutionBackward0]
	140403824368944 -> 140403824349584
	140403824368944 [label=AddBackward0]
	140403824349968 -> 140403824368944
	140403824349968 [label=CudnnBatchNormBackward0]
	140403824350112 -> 140403824349968
	140403824350112 [label=ConvolutionBackward0]
	140403824350304 -> 140403824350112
	140403824350304 [label=ReluBackward0]
	140403824350448 -> 140403824350304
	140403824350448 [label=CudnnBatchNormBackward0]
	140403824350496 -> 140403824350448
	140403824350496 [label=ConvolutionBackward0]
	140403824350784 -> 140403824350496
	140403824350784 [label=ReluBackward0]
	140403824350928 -> 140403824350784
	140403824350928 [label=CudnnBatchNormBackward0]
	140403824350976 -> 140403824350928
	140403824350976 [label=ConvolutionBackward0]
	140403824349920 -> 140403824350976
	140403824349920 [label=CudnnBatchNormBackward0]
	140403824351360 -> 140403824349920
	140403824351360 [label=ConvolutionBackward0]
	140403824351552 -> 140403824351360
	140403824351552 [label=ReluBackward0]
	140403824351696 -> 140403824351552
	140403824351696 [label=CudnnBatchNormBackward0]
	140403824351744 -> 140403824351696
	140403824351744 [label=ConvolutionBackward0]
	140403824352032 -> 140403824351744
	140403824352032 [label=ReluBackward0]
	140403824352176 -> 140403824352032
	140403824352176 [label=CudnnBatchNormBackward0]
	140403824352224 -> 140403824352176
	140403824352224 [label=ConvolutionBackward0]
	140403824352512 -> 140403824352224
	140403824352512 [label=AddBackward0]
	140403824352656 -> 140403824352512
	140403824352656 [label=CudnnBatchNormBackward0]
	140403824352800 -> 140403824352656
	140403824352800 [label=ConvolutionBackward0]
	140403824352992 -> 140403824352800
	140403824352992 [label=ReluBackward0]
	140403824353136 -> 140403824352992
	140403824353136 [label=CudnnBatchNormBackward0]
	140403824353184 -> 140403824353136
	140403824353184 [label=ConvolutionBackward0]
	140403824468224 -> 140403824353184
	140403824468224 [label=ReluBackward0]
	140403824468368 -> 140403824468224
	140403824468368 [label=CudnnBatchNormBackward0]
	140403824468416 -> 140403824468368
	140403824468416 [label=ConvolutionBackward0]
	140403824352608 -> 140403824468416
	140403824352608 [label=AddBackward0]
	140403824468800 -> 140403824352608
	140403824468800 [label=CudnnBatchNormBackward0]
	140403824468944 -> 140403824468800
	140403824468944 [label=ConvolutionBackward0]
	140403824469136 -> 140403824468944
	140403824469136 [label=ReluBackward0]
	140403824469280 -> 140403824469136
	140403824469280 [label=CudnnBatchNormBackward0]
	140403824469328 -> 140403824469280
	140403824469328 [label=ConvolutionBackward0]
	140403824469616 -> 140403824469328
	140403824469616 [label=ReluBackward0]
	140403824469760 -> 140403824469616
	140403824469760 [label=CudnnBatchNormBackward0]
	140403824469808 -> 140403824469760
	140403824469808 [label=ConvolutionBackward0]
	140403824470096 -> 140403824469808
	140403824470096 [label=AddBackward0]
	140403824470240 -> 140403824470096
	140403824470240 [label=CudnnBatchNormBackward0]
	140403824470384 -> 140403824470240
	140403824470384 [label=ConvolutionBackward0]
	140403824470576 -> 140403824470384
	140403824470576 [label=ReluBackward0]
	140403824470720 -> 140403824470576
	140403824470720 [label=CudnnBatchNormBackward0]
	140403824470768 -> 140403824470720
	140403824470768 [label=ConvolutionBackward0]
	140403824471056 -> 140403824470768
	140403824471056 [label=ReluBackward0]
	140403824471200 -> 140403824471056
	140403824471200 [label=CudnnBatchNormBackward0]
	140403824471248 -> 140403824471200
	140403824471248 [label=ConvolutionBackward0]
	140403824471536 -> 140403824471248
	140403824471536 [label=ReluBackward0]
	140403824471680 -> 140403824471536
	140403824471680 [label=CudnnBatchNormBackward0]
	140403824471728 -> 140403824471680
	140403824471728 [label=ConvolutionBackward0]
	140403824472016 -> 140403824471728
	140404615027744 [label="module.conv1.weight
 (32, 3, 3, 3)" fillcolor=lightblue]
	140404615027744 -> 140403824472016
	140403824472016 [label=AccumulateGrad]
	140403824471584 -> 140403824471680
	140404615027664 [label="module.bn1.weight
 (32)" fillcolor=lightblue]
	140404615027664 -> 140403824471584
	140403824471584 [label=AccumulateGrad]
	140403824471824 -> 140403824471680
	140404615027584 [label="module.bn1.bias
 (32)" fillcolor=lightblue]
	140404615027584 -> 140403824471824
	140403824471824 [label=AccumulateGrad]
	140403824471488 -> 140403824471248
	140404615026064 [label="module.layers.0.conv1.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	140404615026064 -> 140403824471488
	140403824471488 [label=AccumulateGrad]
	140403824471104 -> 140403824471200
	140404615025984 [label="module.layers.0.bn1.weight
 (32)" fillcolor=lightblue]
	140404615025984 -> 140403824471104
	140403824471104 [label=AccumulateGrad]
	140403824471344 -> 140403824471200
	140404615027184 [label="module.layers.0.bn1.bias
 (32)" fillcolor=lightblue]
	140404615027184 -> 140403824471344
	140403824471344 [label=AccumulateGrad]
	140403824471008 -> 140403824470768
	140404615072240 [label="module.layers.0.conv2.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140404615072240 -> 140403824471008
	140403824471008 [label=AccumulateGrad]
	140403824470624 -> 140403824470720
	140404615072320 [label="module.layers.0.bn2.weight
 (32)" fillcolor=lightblue]
	140404615072320 -> 140403824470624
	140403824470624 [label=AccumulateGrad]
	140403824470864 -> 140403824470720
	140404615072160 [label="module.layers.0.bn2.bias
 (32)" fillcolor=lightblue]
	140404615072160 -> 140403824470864
	140403824470864 [label=AccumulateGrad]
	140403824470528 -> 140403824470384
	140404615071600 [label="module.layers.0.conv3.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140404615071600 -> 140403824470528
	140403824470528 [label=AccumulateGrad]
	140403824470336 -> 140403824470240
	140404615071520 [label="module.layers.0.bn3.weight
 (16)" fillcolor=lightblue]
	140404615071520 -> 140403824470336
	140403824470336 [label=AccumulateGrad]
	140403824470288 -> 140403824470240
	140404615071440 [label="module.layers.0.bn3.bias
 (16)" fillcolor=lightblue]
	140404615071440 -> 140403824470288
	140403824470288 [label=AccumulateGrad]
	140403824470192 -> 140403824470096
	140403824470192 [label=CudnnBatchNormBackward0]
	140403864643232 -> 140403824470192
	140403864643232 [label=ConvolutionBackward0]
	140403824471536 -> 140403864643232
	140403824470672 -> 140403864643232
	140404615071040 [label="module.layers.0.shortcut.0.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140404615071040 -> 140403824470672
	140403824470672 [label=AccumulateGrad]
	140403864603136 -> 140403824470192
	140404615070960 [label="module.layers.0.shortcut.1.weight
 (16)" fillcolor=lightblue]
	140404615070960 -> 140403864603136
	140403864603136 [label=AccumulateGrad]
	140403864603616 -> 140403824470192
	140404615070880 [label="module.layers.0.shortcut.1.bias
 (16)" fillcolor=lightblue]
	140404615070880 -> 140403864603616
	140403864603616 [label=AccumulateGrad]
	140403824470048 -> 140403824469808
	140404615070480 [label="module.layers.1.conv1.weight
 (112, 16, 1, 1)" fillcolor=lightblue]
	140404615070480 -> 140403824470048
	140403824470048 [label=AccumulateGrad]
	140403824469664 -> 140403824469760
	140404615070400 [label="module.layers.1.bn1.weight
 (112)" fillcolor=lightblue]
	140404615070400 -> 140403824469664
	140403824469664 [label=AccumulateGrad]
	140403824469904 -> 140403824469760
	140404615070320 [label="module.layers.1.bn1.bias
 (112)" fillcolor=lightblue]
	140404615070320 -> 140403824469904
	140403824469904 [label=AccumulateGrad]
	140403824469568 -> 140403824469328
	140404615069840 [label="module.layers.1.conv2.weight
 (112, 1, 3, 3)" fillcolor=lightblue]
	140404615069840 -> 140403824469568
	140403824469568 [label=AccumulateGrad]
	140403824469184 -> 140403824469280
	140404615069920 [label="module.layers.1.bn2.weight
 (112)" fillcolor=lightblue]
	140404615069920 -> 140403824469184
	140403824469184 [label=AccumulateGrad]
	140403824469424 -> 140403824469280
	140404615069760 [label="module.layers.1.bn2.bias
 (112)" fillcolor=lightblue]
	140404615069760 -> 140403824469424
	140403824469424 [label=AccumulateGrad]
	140403824469088 -> 140403824468944
	140404615033776 [label="module.layers.1.conv3.weight
 (24, 112, 1, 1)" fillcolor=lightblue]
	140404615033776 -> 140403824469088
	140403824469088 [label=AccumulateGrad]
	140403824468896 -> 140403824468800
	140404615033696 [label="module.layers.1.bn3.weight
 (24)" fillcolor=lightblue]
	140404615033696 -> 140403824468896
	140403824468896 [label=AccumulateGrad]
	140403824468848 -> 140403824468800
	140404615033616 [label="module.layers.1.bn3.bias
 (24)" fillcolor=lightblue]
	140404615033616 -> 140403824468848
	140403824468848 [label=AccumulateGrad]
	140403824468752 -> 140403824352608
	140403824468752 [label=CudnnBatchNormBackward0]
	140403864601120 -> 140403824468752
	140403864601120 [label=ConvolutionBackward0]
	140403824470096 -> 140403864601120
	140403824469952 -> 140403864601120
	140404615033056 [label="module.layers.1.shortcut.0.weight
 (24, 16, 1, 1)" fillcolor=lightblue]
	140404615033056 -> 140403824469952
	140403824469952 [label=AccumulateGrad]
	140403824469040 -> 140403824468752
	140404615032976 [label="module.layers.1.shortcut.1.weight
 (24)" fillcolor=lightblue]
	140404615032976 -> 140403824469040
	140403824469040 [label=AccumulateGrad]
	140403824468992 -> 140403824468752
	140404615032896 [label="module.layers.1.shortcut.1.bias
 (24)" fillcolor=lightblue]
	140404615032896 -> 140403824468992
	140403824468992 [label=AccumulateGrad]
	140403824468704 -> 140403824468416
	140404615011600 [label="module.layers.2.conv1.weight
 (168, 24, 1, 1)" fillcolor=lightblue]
	140404615011600 -> 140403824468704
	140403824468704 [label=AccumulateGrad]
	140403824468272 -> 140403824468368
	140404615011760 [label="module.layers.2.bn1.weight
 (168)" fillcolor=lightblue]
	140404615011760 -> 140403824468272
	140403824468272 [label=AccumulateGrad]
	140403824468512 -> 140403824468368
	140404615011680 [label="module.layers.2.bn1.bias
 (168)" fillcolor=lightblue]
	140404615011680 -> 140403824468512
	140403824468512 [label=AccumulateGrad]
	140403824468176 -> 140403824353184
	140404615010960 [label="module.layers.2.conv2.weight
 (168, 1, 3, 3)" fillcolor=lightblue]
	140404615010960 -> 140403824468176
	140403824468176 [label=AccumulateGrad]
	140403824353040 -> 140403824353136
	140404615011040 [label="module.layers.2.bn2.weight
 (168)" fillcolor=lightblue]
	140404615011040 -> 140403824353040
	140403824353040 [label=AccumulateGrad]
	140403824468032 -> 140403824353136
	140404615010640 [label="module.layers.2.bn2.bias
 (168)" fillcolor=lightblue]
	140404615010640 -> 140403824468032
	140403824468032 [label=AccumulateGrad]
	140403824352944 -> 140403824352800
	140404615010480 [label="module.layers.2.conv3.weight
 (24, 168, 1, 1)" fillcolor=lightblue]
	140404615010480 -> 140403824352944
	140403824352944 [label=AccumulateGrad]
	140403824352752 -> 140403824352656
	140404615010160 [label="module.layers.2.bn3.weight
 (24)" fillcolor=lightblue]
	140404615010160 -> 140403824352752
	140403824352752 [label=AccumulateGrad]
	140403824352704 -> 140403824352656
	140404615010320 [label="module.layers.2.bn3.bias
 (24)" fillcolor=lightblue]
	140404615010320 -> 140403824352704
	140403824352704 [label=AccumulateGrad]
	140403824352608 -> 140403824352512
	140403824352464 -> 140403824352224
	140404615009680 [label="module.layers.3.conv1.weight
 (168, 24, 1, 1)" fillcolor=lightblue]
	140404615009680 -> 140403824352464
	140403824352464 [label=AccumulateGrad]
	140403824352080 -> 140403824352176
	140404615009840 [label="module.layers.3.bn1.weight
 (168)" fillcolor=lightblue]
	140404615009840 -> 140403824352080
	140403824352080 [label=AccumulateGrad]
	140403824352320 -> 140403824352176
	140404615009760 [label="module.layers.3.bn1.bias
 (168)" fillcolor=lightblue]
	140404615009760 -> 140403824352320
	140403824352320 [label=AccumulateGrad]
	140403824351984 -> 140403824351744
	140404615009280 [label="module.layers.3.conv2.weight
 (168, 1, 3, 3)" fillcolor=lightblue]
	140404615009280 -> 140403824351984
	140403824351984 [label=AccumulateGrad]
	140403824351600 -> 140403824351696
	140404615009360 [label="module.layers.3.bn2.weight
 (168)" fillcolor=lightblue]
	140404615009360 -> 140403824351600
	140403824351600 [label=AccumulateGrad]
	140403824351840 -> 140403824351696
	140404615008960 [label="module.layers.3.bn2.bias
 (168)" fillcolor=lightblue]
	140404615008960 -> 140403824351840
	140403824351840 [label=AccumulateGrad]
	140403824351504 -> 140403824351360
	140404615008800 [label="module.layers.3.conv3.weight
 (32, 168, 1, 1)" fillcolor=lightblue]
	140404615008800 -> 140403824351504
	140403824351504 [label=AccumulateGrad]
	140403824351312 -> 140403824349920
	140404615008480 [label="module.layers.3.bn3.weight
 (32)" fillcolor=lightblue]
	140404615008480 -> 140403824351312
	140403824351312 [label=AccumulateGrad]
	140403824351168 -> 140403824349920
	140404615008640 [label="module.layers.3.bn3.bias
 (32)" fillcolor=lightblue]
	140404615008640 -> 140403824351168
	140403824351168 [label=AccumulateGrad]
	140403824351264 -> 140403824350976
	140404615011280 [label="module.layers.4.conv1.weight
 (224, 32, 1, 1)" fillcolor=lightblue]
	140404615011280 -> 140403824351264
	140403824351264 [label=AccumulateGrad]
	140403824350832 -> 140403824350928
	140404615011200 [label="module.layers.4.bn1.weight
 (224)" fillcolor=lightblue]
	140404615011200 -> 140403824350832
	140403824350832 [label=AccumulateGrad]
	140403824351072 -> 140403824350928
	140404615011840 [label="module.layers.4.bn1.bias
 (224)" fillcolor=lightblue]
	140404615011840 -> 140403824351072
	140403824351072 [label=AccumulateGrad]
	140403824350736 -> 140403824350496
	140403864726640 [label="module.layers.4.conv2.weight
 (224, 1, 3, 3)" fillcolor=lightblue]
	140403864726640 -> 140403824350736
	140403824350736 [label=AccumulateGrad]
	140403824350352 -> 140403824350448
	140403864726560 [label="module.layers.4.bn2.weight
 (224)" fillcolor=lightblue]
	140403864726560 -> 140403824350352
	140403824350352 [label=AccumulateGrad]
	140403824350592 -> 140403824350448
	140403864726480 [label="module.layers.4.bn2.bias
 (224)" fillcolor=lightblue]
	140403864726480 -> 140403824350592
	140403824350592 [label=AccumulateGrad]
	140403824350256 -> 140403824350112
	140403864726880 [label="module.layers.4.conv3.weight
 (32, 224, 1, 1)" fillcolor=lightblue]
	140403864726880 -> 140403824350256
	140403824350256 [label=AccumulateGrad]
	140403824350064 -> 140403824349968
	140403864726720 [label="module.layers.4.bn3.weight
 (32)" fillcolor=lightblue]
	140403864726720 -> 140403824350064
	140403824350064 [label=AccumulateGrad]
	140403824350016 -> 140403824349968
	140403864725200 [label="module.layers.4.bn3.bias
 (32)" fillcolor=lightblue]
	140403864725200 -> 140403824350016
	140403824350016 [label=AccumulateGrad]
	140403824349920 -> 140403824368944
	140403824349872 -> 140403824349584
	140403864689536 [label="module.layers.5.conv1.weight
 (224, 32, 1, 1)" fillcolor=lightblue]
	140403864689536 -> 140403824349872
	140403824349872 [label=AccumulateGrad]
	140403824349440 -> 140403824349536
	140403864688816 [label="module.layers.5.bn1.weight
 (224)" fillcolor=lightblue]
	140403864688816 -> 140403824349440
	140403824349440 [label=AccumulateGrad]
	140403824349680 -> 140403824349536
	140403864689296 [label="module.layers.5.bn1.bias
 (224)" fillcolor=lightblue]
	140403864689296 -> 140403824349680
	140403824349680 [label=AccumulateGrad]
	140403824349344 -> 140403824369520
	140403864689776 [label="module.layers.5.conv2.weight
 (224, 1, 3, 3)" fillcolor=lightblue]
	140403864689776 -> 140403824349344
	140403824349344 [label=AccumulateGrad]
	140403824369376 -> 140403824369472
	140403864689696 [label="module.layers.5.bn2.weight
 (224)" fillcolor=lightblue]
	140403864689696 -> 140403824369376
	140403824369376 [label=AccumulateGrad]
	140403824369616 -> 140403824369472
	140403864689856 [label="module.layers.5.bn2.bias
 (224)" fillcolor=lightblue]
	140403864689856 -> 140403824369616
	140403824369616 [label=AccumulateGrad]
	140403824369280 -> 140403824369136
	140403864690256 [label="module.layers.5.conv3.weight
 (32, 224, 1, 1)" fillcolor=lightblue]
	140403864690256 -> 140403824369280
	140403824369280 [label=AccumulateGrad]
	140403824369088 -> 140403824368992
	140403864690416 [label="module.layers.5.bn3.weight
 (32)" fillcolor=lightblue]
	140403864690416 -> 140403824369088
	140403824369088 [label=AccumulateGrad]
	140403824369040 -> 140403824368992
	140403864690496 [label="module.layers.5.bn3.bias
 (32)" fillcolor=lightblue]
	140403864690496 -> 140403824369040
	140403824369040 [label=AccumulateGrad]
	140403824368944 -> 140403824368848
	140403824368800 -> 140403824368560
	140403864699264 [label="module.layers.6.conv1.weight
 (224, 32, 1, 1)" fillcolor=lightblue]
	140403864699264 -> 140403824368800
	140403824368800 [label=AccumulateGrad]
	140403824368416 -> 140403824368512
	140403864699184 [label="module.layers.6.bn1.weight
 (224)" fillcolor=lightblue]
	140403864699184 -> 140403824368416
	140403824368416 [label=AccumulateGrad]
	140403824368656 -> 140403824368512
	140403864699344 [label="module.layers.6.bn1.bias
 (224)" fillcolor=lightblue]
	140403864699344 -> 140403824368656
	140403824368656 [label=AccumulateGrad]
	140403824368320 -> 140403824368080
	140403864699824 [label="module.layers.6.conv2.weight
 (224, 1, 3, 3)" fillcolor=lightblue]
	140403864699824 -> 140403824368320
	140403824368320 [label=AccumulateGrad]
	140403824367936 -> 140403824368032
	140403864699744 [label="module.layers.6.bn2.weight
 (224)" fillcolor=lightblue]
	140403864699744 -> 140403824367936
	140403824367936 [label=AccumulateGrad]
	140403824368176 -> 140403824368032
	140403864699904 [label="module.layers.6.bn2.bias
 (224)" fillcolor=lightblue]
	140403864699904 -> 140403824368176
	140403824368176 [label=AccumulateGrad]
	140403824367840 -> 140403824367696
	140403864700304 [label="module.layers.6.conv3.weight
 (64, 224, 1, 1)" fillcolor=lightblue]
	140403864700304 -> 140403824367840
	140403824367840 [label=AccumulateGrad]
	140403824367648 -> 140403824366256
	140403864700384 [label="module.layers.6.bn3.weight
 (64)" fillcolor=lightblue]
	140403864700384 -> 140403824367648
	140403824367648 [label=AccumulateGrad]
	140403824367504 -> 140403824366256
	140403864700464 [label="module.layers.6.bn3.bias
 (64)" fillcolor=lightblue]
	140403864700464 -> 140403824367504
	140403824367504 [label=AccumulateGrad]
	140403824367600 -> 140403824367312
	140403864700864 [label="module.layers.7.conv1.weight
 (448, 64, 1, 1)" fillcolor=lightblue]
	140403864700864 -> 140403824367600
	140403824367600 [label=AccumulateGrad]
	140403824367168 -> 140403824367264
	140403864700944 [label="module.layers.7.bn1.weight
 (448)" fillcolor=lightblue]
	140403864700944 -> 140403824367168
	140403824367168 [label=AccumulateGrad]
	140403824367408 -> 140403824367264
	140403864701024 [label="module.layers.7.bn1.bias
 (448)" fillcolor=lightblue]
	140403864701024 -> 140403824367408
	140403824367408 [label=AccumulateGrad]
	140403824367072 -> 140403824366832
	140403864701504 [label="module.layers.7.conv2.weight
 (448, 1, 3, 3)" fillcolor=lightblue]
	140403864701504 -> 140403824367072
	140403824367072 [label=AccumulateGrad]
	140403824366688 -> 140403824366784
	140403864701424 [label="module.layers.7.bn2.weight
 (448)" fillcolor=lightblue]
	140403864701424 -> 140403824366688
	140403824366688 [label=AccumulateGrad]
	140403824366928 -> 140403824366784
	140403864701584 [label="module.layers.7.bn2.bias
 (448)" fillcolor=lightblue]
	140403864701584 -> 140403824366928
	140403824366928 [label=AccumulateGrad]
	140403824366592 -> 140403824366448
	140403864701984 [label="module.layers.7.conv3.weight
 (64, 448, 1, 1)" fillcolor=lightblue]
	140403864701984 -> 140403824366592
	140403824366592 [label=AccumulateGrad]
	140403824366400 -> 140403824366304
	140403864702064 [label="module.layers.7.bn3.weight
 (64)" fillcolor=lightblue]
	140403864702064 -> 140403824366400
	140403824366400 [label=AccumulateGrad]
	140403824366352 -> 140403824366304
	140403864702144 [label="module.layers.7.bn3.bias
 (64)" fillcolor=lightblue]
	140403864702144 -> 140403824366352
	140403824366352 [label=AccumulateGrad]
	140403824366256 -> 140403824381184
	140403824366208 -> 140403824365920
	140403864702544 [label="module.layers.8.conv1.weight
 (448, 64, 1, 1)" fillcolor=lightblue]
	140403864702544 -> 140403824366208
	140403824366208 [label=AccumulateGrad]
	140403824365776 -> 140403824365872
	140403864702624 [label="module.layers.8.bn1.weight
 (448)" fillcolor=lightblue]
	140403864702624 -> 140403824365776
	140403824365776 [label=AccumulateGrad]
	140403824366016 -> 140403824365872
	140403864702704 [label="module.layers.8.bn1.bias
 (448)" fillcolor=lightblue]
	140403864702704 -> 140403824366016
	140403824366016 [label=AccumulateGrad]
	140403824365680 -> 140403824381760
	140403847725360 [label="module.layers.8.conv2.weight
 (448, 1, 3, 3)" fillcolor=lightblue]
	140403847725360 -> 140403824365680
	140403824365680 [label=AccumulateGrad]
	140403824381616 -> 140403824381712
	140403847725280 [label="module.layers.8.bn2.weight
 (448)" fillcolor=lightblue]
	140403847725280 -> 140403824381616
	140403824381616 [label=AccumulateGrad]
	140403824381856 -> 140403824381712
	140403847725440 [label="module.layers.8.bn2.bias
 (448)" fillcolor=lightblue]
	140403847725440 -> 140403824381856
	140403824381856 [label=AccumulateGrad]
	140403824381520 -> 140403824381376
	140403847725840 [label="module.layers.8.conv3.weight
 (64, 448, 1, 1)" fillcolor=lightblue]
	140403847725840 -> 140403824381520
	140403824381520 [label=AccumulateGrad]
	140403824381328 -> 140403824381232
	140403847725920 [label="module.layers.8.bn3.weight
 (64)" fillcolor=lightblue]
	140403847725920 -> 140403824381328
	140403824381328 [label=AccumulateGrad]
	140403824381280 -> 140403824381232
	140403847726000 [label="module.layers.8.bn3.bias
 (64)" fillcolor=lightblue]
	140403847726000 -> 140403824381280
	140403824381280 [label=AccumulateGrad]
	140403824381184 -> 140403824379792
	140403824381136 -> 140403824380848
	140403847726400 [label="module.layers.9.conv1.weight
 (448, 64, 1, 1)" fillcolor=lightblue]
	140403847726400 -> 140403824381136
	140403824381136 [label=AccumulateGrad]
	140403824380704 -> 140403824380800
	140403847726480 [label="module.layers.9.bn1.weight
 (448)" fillcolor=lightblue]
	140403847726480 -> 140403824380704
	140403824380704 [label=AccumulateGrad]
	140403824380944 -> 140403824380800
	140403847726560 [label="module.layers.9.bn1.bias
 (448)" fillcolor=lightblue]
	140403847726560 -> 140403824380944
	140403824380944 [label=AccumulateGrad]
	140403824380608 -> 140403824380368
	140403847727040 [label="module.layers.9.conv2.weight
 (448, 1, 3, 3)" fillcolor=lightblue]
	140403847727040 -> 140403824380608
	140403824380608 [label=AccumulateGrad]
	140403824380224 -> 140403824380320
	140403847726960 [label="module.layers.9.bn2.weight
 (448)" fillcolor=lightblue]
	140403847726960 -> 140403824380224
	140403824380224 [label=AccumulateGrad]
	140403824380464 -> 140403824380320
	140403847727120 [label="module.layers.9.bn2.bias
 (448)" fillcolor=lightblue]
	140403847727120 -> 140403824380464
	140403824380464 [label=AccumulateGrad]
	140403824380128 -> 140403824379984
	140403847727520 [label="module.layers.9.conv3.weight
 (64, 448, 1, 1)" fillcolor=lightblue]
	140403847727520 -> 140403824380128
	140403824380128 [label=AccumulateGrad]
	140403824379936 -> 140403824379840
	140403847727600 [label="module.layers.9.bn3.weight
 (64)" fillcolor=lightblue]
	140403847727600 -> 140403824379936
	140403824379936 [label=AccumulateGrad]
	140403824379888 -> 140403824379840
	140403847727680 [label="module.layers.9.bn3.bias
 (64)" fillcolor=lightblue]
	140403847727680 -> 140403824379888
	140403824379888 [label=AccumulateGrad]
	140403824379792 -> 140403824379696
	140403824379648 -> 140403824379408
	140403847728080 [label="module.layers.10.conv1.weight
 (448, 64, 1, 1)" fillcolor=lightblue]
	140403847728080 -> 140403824379648
	140403824379648 [label=AccumulateGrad]
	140403824379264 -> 140403824379360
	140403847728160 [label="module.layers.10.bn1.weight
 (448)" fillcolor=lightblue]
	140403847728160 -> 140403824379264
	140403824379264 [label=AccumulateGrad]
	140403824379504 -> 140403824379360
	140403847728240 [label="module.layers.10.bn1.bias
 (448)" fillcolor=lightblue]
	140403847728240 -> 140403824379504
	140403824379504 [label=AccumulateGrad]
	140403824379168 -> 140403824378928
	140403847728720 [label="module.layers.10.conv2.weight
 (448, 1, 3, 3)" fillcolor=lightblue]
	140403847728720 -> 140403824379168
	140403824379168 [label=AccumulateGrad]
	140403824378784 -> 140403824378880
	140403847728640 [label="module.layers.10.bn2.weight
 (448)" fillcolor=lightblue]
	140403847728640 -> 140403824378784
	140403824378784 [label=AccumulateGrad]
	140403824379024 -> 140403824378880
	140403847728800 [label="module.layers.10.bn2.bias
 (448)" fillcolor=lightblue]
	140403847728800 -> 140403824379024
	140403824379024 [label=AccumulateGrad]
	140403824378688 -> 140403824378544
	140403847295120 [label="module.layers.10.conv3.weight
 (96, 448, 1, 1)" fillcolor=lightblue]
	140403847295120 -> 140403824378688
	140403824378688 [label=AccumulateGrad]
	140403824378496 -> 140403824378400
	140403847295200 [label="module.layers.10.bn3.weight
 (96)" fillcolor=lightblue]
	140403847295200 -> 140403824378496
	140403824378496 [label=AccumulateGrad]
	140403824378448 -> 140403824378400
	140403847295280 [label="module.layers.10.bn3.bias
 (96)" fillcolor=lightblue]
	140403847295280 -> 140403824378448
	140403824378448 [label=AccumulateGrad]
	140403824378352 -> 140403824401472
	140403824378352 [label=CudnnBatchNormBackward0]
	140403864601744 -> 140403824378352
	140403864601744 [label=ConvolutionBackward0]
	140403824379696 -> 140403864601744
	140403824589936 -> 140403864601744
	140403847295680 [label="module.layers.10.shortcut.0.weight
 (96, 64, 1, 1)" fillcolor=lightblue]
	140403847295680 -> 140403824589936
	140403824589936 [label=AccumulateGrad]
	140403864601936 -> 140403824378352
	140403847295760 [label="module.layers.10.shortcut.1.weight
 (96)" fillcolor=lightblue]
	140403847295760 -> 140403864601936
	140403864601936 [label=AccumulateGrad]
	140403864600976 -> 140403824378352
	140403847295840 [label="module.layers.10.shortcut.1.bias
 (96)" fillcolor=lightblue]
	140403847295840 -> 140403864600976
	140403864600976 [label=AccumulateGrad]
	140403824378304 -> 140403824378016
	140403847296240 [label="module.layers.11.conv1.weight
 (672, 96, 1, 1)" fillcolor=lightblue]
	140403847296240 -> 140403824378304
	140403824378304 [label=AccumulateGrad]
	140403824377920 -> 140403824402384
	140403847296320 [label="module.layers.11.bn1.weight
 (672)" fillcolor=lightblue]
	140403847296320 -> 140403824377920
	140403824377920 [label=AccumulateGrad]
	140403824378112 -> 140403824402384
	140403847296400 [label="module.layers.11.bn1.bias
 (672)" fillcolor=lightblue]
	140403847296400 -> 140403824378112
	140403824378112 [label=AccumulateGrad]
	140403824402288 -> 140403824402048
	140403847296880 [label="module.layers.11.conv2.weight
 (672, 1, 3, 3)" fillcolor=lightblue]
	140403847296880 -> 140403824402288
	140403824402288 [label=AccumulateGrad]
	140403824401904 -> 140403824402000
	140403847296800 [label="module.layers.11.bn2.weight
 (672)" fillcolor=lightblue]
	140403847296800 -> 140403824401904
	140403824401904 [label=AccumulateGrad]
	140403824402144 -> 140403824402000
	140403847296960 [label="module.layers.11.bn2.bias
 (672)" fillcolor=lightblue]
	140403847296960 -> 140403824402144
	140403824402144 [label=AccumulateGrad]
	140403824401808 -> 140403824401664
	140403847297360 [label="module.layers.11.conv3.weight
 (96, 672, 1, 1)" fillcolor=lightblue]
	140403847297360 -> 140403824401808
	140403824401808 [label=AccumulateGrad]
	140403824401616 -> 140403824401520
	140403847297440 [label="module.layers.11.bn3.weight
 (96)" fillcolor=lightblue]
	140403847297440 -> 140403824401616
	140403824401616 [label=AccumulateGrad]
	140403824401568 -> 140403824401520
	140403847297520 [label="module.layers.11.bn3.bias
 (96)" fillcolor=lightblue]
	140403847297520 -> 140403824401568
	140403824401568 [label=AccumulateGrad]
	140403824401472 -> 140403824400080
	140403824401424 -> 140403824401136
	140403847297920 [label="module.layers.12.conv1.weight
 (672, 96, 1, 1)" fillcolor=lightblue]
	140403847297920 -> 140403824401424
	140403824401424 [label=AccumulateGrad]
	140403824400992 -> 140403824401088
	140403847298000 [label="module.layers.12.bn1.weight
 (672)" fillcolor=lightblue]
	140403847298000 -> 140403824400992
	140403824400992 [label=AccumulateGrad]
	140403824401232 -> 140403824401088
	140403847298080 [label="module.layers.12.bn1.bias
 (672)" fillcolor=lightblue]
	140403847298080 -> 140403824401232
	140403824401232 [label=AccumulateGrad]
	140403824400896 -> 140403824400656
	140403847298560 [label="module.layers.12.conv2.weight
 (672, 1, 3, 3)" fillcolor=lightblue]
	140403847298560 -> 140403824400896
	140403824400896 [label=AccumulateGrad]
	140403824400512 -> 140403824400608
	140403847298480 [label="module.layers.12.bn2.weight
 (672)" fillcolor=lightblue]
	140403847298480 -> 140403824400512
	140403824400512 [label=AccumulateGrad]
	140403824400752 -> 140403824400608
	140403847298640 [label="module.layers.12.bn2.bias
 (672)" fillcolor=lightblue]
	140403847298640 -> 140403824400752
	140403824400752 [label=AccumulateGrad]
	140403824400416 -> 140403824400272
	140403847385152 [label="module.layers.12.conv3.weight
 (96, 672, 1, 1)" fillcolor=lightblue]
	140403847385152 -> 140403824400416
	140403824400416 [label=AccumulateGrad]
	140403824400224 -> 140403824400128
	140403847385232 [label="module.layers.12.bn3.weight
 (96)" fillcolor=lightblue]
	140403847385232 -> 140403824400224
	140403824400224 [label=AccumulateGrad]
	140403824400176 -> 140403824400128
	140403847385312 [label="module.layers.12.bn3.bias
 (96)" fillcolor=lightblue]
	140403847385312 -> 140403824400176
	140403824400176 [label=AccumulateGrad]
	140403824400080 -> 140403824399984
	140403824399936 -> 140403824399696
	140403847385712 [label="module.layers.13.conv1.weight
 (672, 96, 1, 1)" fillcolor=lightblue]
	140403847385712 -> 140403824399936
	140403824399936 [label=AccumulateGrad]
	140403824399552 -> 140403824399648
	140403847385792 [label="module.layers.13.bn1.weight
 (672)" fillcolor=lightblue]
	140403847385792 -> 140403824399552
	140403824399552 [label=AccumulateGrad]
	140403824399792 -> 140403824399648
	140403847385872 [label="module.layers.13.bn1.bias
 (672)" fillcolor=lightblue]
	140403847385872 -> 140403824399792
	140403824399792 [label=AccumulateGrad]
	140403824399456 -> 140403824399216
	140403847386352 [label="module.layers.13.conv2.weight
 (672, 1, 3, 3)" fillcolor=lightblue]
	140403847386352 -> 140403824399456
	140403824399456 [label=AccumulateGrad]
	140403824399072 -> 140403824399168
	140403847386272 [label="module.layers.13.bn2.weight
 (672)" fillcolor=lightblue]
	140403847386272 -> 140403824399072
	140403824399072 [label=AccumulateGrad]
	140403824399312 -> 140403824399168
	140403847386432 [label="module.layers.13.bn2.bias
 (672)" fillcolor=lightblue]
	140403847386432 -> 140403824399312
	140403824399312 [label=AccumulateGrad]
	140403824398976 -> 140403824398832
	140403847386832 [label="module.layers.13.conv3.weight
 (160, 672, 1, 1)" fillcolor=lightblue]
	140403847386832 -> 140403824398976
	140403824398976 [label=AccumulateGrad]
	140403824398784 -> 140403824397328
	140403847386912 [label="module.layers.13.bn3.weight
 (160)" fillcolor=lightblue]
	140403847386912 -> 140403824398784
	140403824398784 [label=AccumulateGrad]
	140403824398640 -> 140403824397328
	140403847386992 [label="module.layers.13.bn3.bias
 (160)" fillcolor=lightblue]
	140403847386992 -> 140403824398640
	140403824398640 [label=AccumulateGrad]
	140403824398736 -> 140403824398448
	140403847387392 [label="module.layers.14.conv1.weight
 (1120, 160, 1, 1)" fillcolor=lightblue]
	140403847387392 -> 140403824398736
	140403824398736 [label=AccumulateGrad]
	140403824398400 -> 140403824398288
	140403847387472 [label="module.layers.14.bn1.weight
 (1120)" fillcolor=lightblue]
	140403847387472 -> 140403824398400
	140403824398400 [label=AccumulateGrad]
	140403824398544 -> 140403824398288
	140403847387552 [label="module.layers.14.bn1.bias
 (1120)" fillcolor=lightblue]
	140403847387552 -> 140403824398544
	140403824398544 [label=AccumulateGrad]
	140403824398144 -> 140403824397904
	140403847388032 [label="module.layers.14.conv2.weight
 (1120, 1, 3, 3)" fillcolor=lightblue]
	140403847388032 -> 140403824398144
	140403824398144 [label=AccumulateGrad]
	140403824397760 -> 140403824397856
	140403847387952 [label="module.layers.14.bn2.weight
 (1120)" fillcolor=lightblue]
	140403847387952 -> 140403824397760
	140403824397760 [label=AccumulateGrad]
	140403824398000 -> 140403824397856
	140403847388112 [label="module.layers.14.bn2.bias
 (1120)" fillcolor=lightblue]
	140403847388112 -> 140403824398000
	140403824398000 [label=AccumulateGrad]
	140403824397664 -> 140403824397520
	140403847388512 [label="module.layers.14.conv3.weight
 (160, 1120, 1, 1)" fillcolor=lightblue]
	140403847388512 -> 140403824397664
	140403824397664 [label=AccumulateGrad]
	140403824397472 -> 140403824397376
	140403847388592 [label="module.layers.14.bn3.weight
 (160)" fillcolor=lightblue]
	140403847388592 -> 140403824397472
	140403824397472 [label=AccumulateGrad]
	140403824397424 -> 140403824397376
	140403847388672 [label="module.layers.14.bn3.bias
 (160)" fillcolor=lightblue]
	140403847388672 -> 140403824397424
	140403824397424 [label=AccumulateGrad]
	140403824397328 -> 140403824395936
	140403824397280 -> 140403824396992
	140403847389072 [label="module.layers.15.conv1.weight
 (1120, 160, 1, 1)" fillcolor=lightblue]
	140403847389072 -> 140403824397280
	140403824397280 [label=AccumulateGrad]
	140403824396848 -> 140403824396944
	140403846963264 [label="module.layers.15.bn1.weight
 (1120)" fillcolor=lightblue]
	140403846963264 -> 140403824396848
	140403824396848 [label=AccumulateGrad]
	140403824397088 -> 140403824396944
	140403846963344 [label="module.layers.15.bn1.bias
 (1120)" fillcolor=lightblue]
	140403846963344 -> 140403824397088
	140403824397088 [label=AccumulateGrad]
	140403824396752 -> 140403824396512
	140403846963824 [label="module.layers.15.conv2.weight
 (1120, 1, 3, 3)" fillcolor=lightblue]
	140403846963824 -> 140403824396752
	140403824396752 [label=AccumulateGrad]
	140403824396368 -> 140403824396464
	140403846963744 [label="module.layers.15.bn2.weight
 (1120)" fillcolor=lightblue]
	140403846963744 -> 140403824396368
	140403824396368 [label=AccumulateGrad]
	140403824396608 -> 140403824396464
	140403846963904 [label="module.layers.15.bn2.bias
 (1120)" fillcolor=lightblue]
	140403846963904 -> 140403824396608
	140403824396608 [label=AccumulateGrad]
	140403824396272 -> 140403824396128
	140403846964304 [label="module.layers.15.conv3.weight
 (160, 1120, 1, 1)" fillcolor=lightblue]
	140403846964304 -> 140403824396272
	140403824396272 [label=AccumulateGrad]
	140403824396080 -> 140403824395984
	140403846964384 [label="module.layers.15.bn3.weight
 (160)" fillcolor=lightblue]
	140403846964384 -> 140403824396080
	140403824396080 [label=AccumulateGrad]
	140403824396032 -> 140403824395984
	140403846964464 [label="module.layers.15.bn3.bias
 (160)" fillcolor=lightblue]
	140403846964464 -> 140403824396032
	140403824396032 [label=AccumulateGrad]
	140403824395936 -> 140403824395840
	140403824395792 -> 140403824395552
	140403846964864 [label="module.layers.16.conv1.weight
 (1120, 160, 1, 1)" fillcolor=lightblue]
	140403846964864 -> 140403824395792
	140403824395792 [label=AccumulateGrad]
	140403824395408 -> 140403824395504
	140403846964944 [label="module.layers.16.bn1.weight
 (1120)" fillcolor=lightblue]
	140403846964944 -> 140403824395408
	140403824395408 [label=AccumulateGrad]
	140403824395648 -> 140403824395504
	140403846965024 [label="module.layers.16.bn1.bias
 (1120)" fillcolor=lightblue]
	140403846965024 -> 140403824395648
	140403824395648 [label=AccumulateGrad]
	140403824395312 -> 140403824395072
	140403846965504 [label="module.layers.16.conv2.weight
 (1120, 1, 3, 3)" fillcolor=lightblue]
	140403846965504 -> 140403824395312
	140403824395312 [label=AccumulateGrad]
	140403824394928 -> 140403824395024
	140403846965424 [label="module.layers.16.bn2.weight
 (1120)" fillcolor=lightblue]
	140403846965424 -> 140403824394928
	140403824394928 [label=AccumulateGrad]
	140403824395168 -> 140403824395024
	140403846965584 [label="module.layers.16.bn2.bias
 (1120)" fillcolor=lightblue]
	140403846965584 -> 140403824395168
	140403824395168 [label=AccumulateGrad]
	140403824394832 -> 140403824394688
	140403846965984 [label="module.layers.16.conv3.weight
 (320, 1120, 1, 1)" fillcolor=lightblue]
	140403846965984 -> 140403824394832
	140403824394832 [label=AccumulateGrad]
	140403824394640 -> 140403824394544
	140403846966064 [label="module.layers.16.bn3.weight
 (320)" fillcolor=lightblue]
	140403846966064 -> 140403824394640
	140403824394640 [label=AccumulateGrad]
	140403824394592 -> 140403824394544
	140403846966144 [label="module.layers.16.bn3.bias
 (320)" fillcolor=lightblue]
	140403846966144 -> 140403824394592
	140403824394592 [label=AccumulateGrad]
	140403824394496 -> 140403824394400
	140403824394496 [label=CudnnBatchNormBackward0]
	140403824395264 -> 140403824394496
	140403824395264 [label=ConvolutionBackward0]
	140403824395840 -> 140403824395264
	140403824395456 -> 140403824395264
	140403846966544 [label="module.layers.16.shortcut.0.weight
 (320, 160, 1, 1)" fillcolor=lightblue]
	140403846966544 -> 140403824395456
	140403824395456 [label=AccumulateGrad]
	140403824394784 -> 140403824394496
	140403846966624 [label="module.layers.16.shortcut.1.weight
 (320)" fillcolor=lightblue]
	140403846966624 -> 140403824394784
	140403824394784 [label=AccumulateGrad]
	140403824394736 -> 140403824394496
	140403846966704 [label="module.layers.16.shortcut.1.bias
 (320)" fillcolor=lightblue]
	140403846966704 -> 140403824394736
	140403824394736 [label=AccumulateGrad]
	140403824394352 -> 140403864602368
	140403846967104 [label="module.conv2.weight
 (1280, 320, 1, 1)" fillcolor=lightblue]
	140403846967104 -> 140403824394352
	140403824394352 [label=AccumulateGrad]
	140403864601984 -> 140403864602464
	140403846967184 [label="module.bn2.weight
 (1280)" fillcolor=lightblue]
	140403846967184 -> 140403864601984
	140403864601984 [label=AccumulateGrad]
	140403864603760 -> 140403864602464
	140403847057472 [label="module.bn2.bias
 (1280)" fillcolor=lightblue]
	140403847057472 -> 140403864603760
	140403864603760 [label=AccumulateGrad]
	140403824589600 -> 140403824590800
	140403824589600 [label=TBackward0]
	140403864603952 -> 140403824589600
	140403847057792 [label="module.linear.weight
 (10, 1280)" fillcolor=lightblue]
	140403847057792 -> 140403864603952
	140403864603952 [label=AccumulateGrad]
	140403824589264 -> 140403847059072
}
