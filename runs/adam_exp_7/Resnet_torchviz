digraph {
	graph [size="156.75,156.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140371899680704 [label="
 ()" fillcolor=darkolivegreen1]
	140371954554912 [label=MeanBackward0]
	140371954555488 -> 140371954554912
	140371954555488 [label=AddmmBackward0]
	140371954555296 -> 140371954555488
	140371899680224 [label="module.linear.bias
 (10)" fillcolor=lightblue]
	140371899680224 -> 140371954555296
	140371954555296 [label=AccumulateGrad]
	140371954554096 -> 140371954555488
	140371954554096 [label=ViewBackward0]
	140371954554768 -> 140371954554096
	140371954554768 [label=AvgPool2DBackward0]
	140371954472992 -> 140371954554768
	140371954472992 [label=ReluBackward0]
	140371954473520 -> 140371954472992
	140371954473520 [label=CudnnBatchNormBackward0]
	140371954473616 -> 140371954473520
	140371954473616 [label=ConvolutionBackward0]
	140371954473952 -> 140371954473616
	140371954473952 [label=AddBackward0]
	140371954473856 -> 140371954473952
	140371954473856 [label=CudnnBatchNormBackward0]
	140371954474528 -> 140371954473856
	140371954474528 [label=ConvolutionBackward0]
	140371954474288 -> 140371954474528
	140371954474288 [label=ReluBackward0]
	140371954474864 -> 140371954474288
	140371954474864 [label=CudnnBatchNormBackward0]
	140371954474816 -> 140371954474864
	140371954474816 [label=ConvolutionBackward0]
	140371954474672 -> 140371954474816
	140371954474672 [label=ReluBackward0]
	140371954472944 -> 140371954474672
	140371954472944 [label=CudnnBatchNormBackward0]
	140371954473040 -> 140371954472944
	140371954473040 [label=ConvolutionBackward0]
	140371954473376 -> 140371954473040
	140371954473376 [label=AddBackward0]
	140371954473088 -> 140371954473376
	140371954473088 [label=CudnnBatchNormBackward0]
	140371954472656 -> 140371954473088
	140371954472656 [label=ConvolutionBackward0]
	140371954472512 -> 140371954472656
	140371954472512 [label=ReluBackward0]
	140371954472032 -> 140371954472512
	140371954472032 [label=CudnnBatchNormBackward0]
	140371954471504 -> 140371954472032
	140371954471504 [label=ConvolutionBackward0]
	140371954471744 -> 140371954471504
	140371954471744 [label=ReluBackward0]
	140371954471072 -> 140371954471744
	140371954471072 [label=CudnnBatchNormBackward0]
	140371954471408 -> 140371954471072
	140371954471408 [label=ConvolutionBackward0]
	140371954473280 -> 140371954471408
	140371954473280 [label=AddBackward0]
	140371954471984 -> 140371954473280
	140371954471984 [label=CudnnBatchNormBackward0]
	140371954471888 -> 140371954471984
	140371954471888 [label=ConvolutionBackward0]
	140371954915072 -> 140371954471888
	140371954915072 [label=ReluBackward0]
	140371954915696 -> 140371954915072
	140371954915696 [label=CudnnBatchNormBackward0]
	140371954917040 -> 140371954915696
	140371954917040 [label=ConvolutionBackward0]
	140371954917184 -> 140371954917040
	140371954917184 [label=ReluBackward0]
	140371954916608 -> 140371954917184
	140371954916608 [label=CudnnBatchNormBackward0]
	140371954916944 -> 140371954916608
	140371954916944 [label=ConvolutionBackward0]
	140371954470976 -> 140371954916944
	140371954470976 [label=CudnnBatchNormBackward0]
	140371954916464 -> 140371954470976
	140371954916464 [label=ConvolutionBackward0]
	140371954916320 -> 140371954916464
	140371954916320 [label=ReluBackward0]
	140371954916032 -> 140371954916320
	140371954916032 [label=CudnnBatchNormBackward0]
	140371954915840 -> 140371954916032
	140371954915840 [label=ConvolutionBackward0]
	140371954915360 -> 140371954915840
	140371954915360 [label=ReluBackward0]
	140371954915408 -> 140371954915360
	140371954915408 [label=CudnnBatchNormBackward0]
	140371954915456 -> 140371954915408
	140371954915456 [label=ConvolutionBackward0]
	140371954915024 -> 140371954915456
	140371954915024 [label=AddBackward0]
	140371954914784 -> 140371954915024
	140371954914784 [label=CudnnBatchNormBackward0]
	140371954914592 -> 140371954914784
	140371954914592 [label=ConvolutionBackward0]
	140371954914352 -> 140371954914592
	140371954914352 [label=ReluBackward0]
	140371954914160 -> 140371954914352
	140371954914160 [label=CudnnBatchNormBackward0]
	140371954913584 -> 140371954914160
	140371954913584 [label=ConvolutionBackward0]
	140371954913824 -> 140371954913584
	140371954913824 [label=ReluBackward0]
	140371954913440 -> 140371954913824
	140371954913440 [label=CudnnBatchNormBackward0]
	140371954913344 -> 140371954913440
	140371954913344 [label=ConvolutionBackward0]
	140371954914736 -> 140371954913344
	140371954914736 [label=AddBackward0]
	140371954914832 -> 140371954914736
	140371954914832 [label=CudnnBatchNormBackward0]
	140371954833968 -> 140371954914832
	140371954833968 [label=ConvolutionBackward0]
	140371954834016 -> 140371954833968
	140371954834016 [label=ReluBackward0]
	140371954835408 -> 140371954834016
	140371954835408 [label=CudnnBatchNormBackward0]
	140371954833920 -> 140371954835408
	140371954833920 [label=ConvolutionBackward0]
	140371954834928 -> 140371954833920
	140371954834928 [label=ReluBackward0]
	140371954835024 -> 140371954834928
	140371954835024 [label=CudnnBatchNormBackward0]
	140371954834448 -> 140371954835024
	140371954834448 [label=ConvolutionBackward0]
	140371954914928 -> 140371954834448
	140371954914928 [label=AddBackward0]
	140371954834544 -> 140371954914928
	140371954834544 [label=CudnnBatchNormBackward0]
	140371954834352 -> 140371954834544
	140371954834352 [label=ConvolutionBackward0]
	140371954834160 -> 140371954834352
	140371954834160 [label=ReluBackward0]
	140371954833824 -> 140371954834160
	140371954833824 [label=CudnnBatchNormBackward0]
	140371954833536 -> 140371954833824
	140371954833536 [label=ConvolutionBackward0]
	140371954833344 -> 140371954833536
	140371954833344 [label=ReluBackward0]
	140371954833152 -> 140371954833344
	140371954833152 [label=CudnnBatchNormBackward0]
	140371954832624 -> 140371954833152
	140371954832624 [label=ConvolutionBackward0]
	140371954832816 -> 140371954832624
	140371954832816 [label=AddBackward0]
	140371954833488 -> 140371954832816
	140371954833488 [label=CudnnBatchNormBackward0]
	140371954833392 -> 140371954833488
	140371954833392 [label=ConvolutionBackward0]
	140371954832048 -> 140371954833392
	140371954832048 [label=ReluBackward0]
	140371954832000 -> 140371954832048
	140371954832000 [label=CudnnBatchNormBackward0]
	140371954831760 -> 140371954832000
	140371954831760 [label=ConvolutionBackward0]
	140371954831424 -> 140371954831760
	140371954831424 [label=ReluBackward0]
	140371954833680 -> 140371954831424
	140371954833680 [label=CudnnBatchNormBackward0]
	140371954833632 -> 140371954833680
	140371954833632 [label=ConvolutionBackward0]
	140371954832768 -> 140371954833632
	140371954832768 [label=AddBackward0]
	140371954748960 -> 140371954832768
	140371954748960 [label=CudnnBatchNormBackward0]
	140371954745936 -> 140371954748960
	140371954745936 [label=ConvolutionBackward0]
	140371954749104 -> 140371954745936
	140371954749104 [label=ReluBackward0]
	140371954749200 -> 140371954749104
	140371954749200 [label=CudnnBatchNormBackward0]
	140371954748576 -> 140371954749200
	140371954748576 [label=ConvolutionBackward0]
	140371954748816 -> 140371954748576
	140371954748816 [label=ReluBackward0]
	140371954746368 -> 140371954748816
	140371954746368 [label=CudnnBatchNormBackward0]
	140371954748432 -> 140371954746368
	140371954748432 [label=ConvolutionBackward0]
	140371954747664 -> 140371954748432
	140371954747664 [label=AddBackward0]
	140371954747760 -> 140371954747664
	140371954747760 [label=CudnnBatchNormBackward0]
	140371954747808 -> 140371954747760
	140371954747808 [label=ConvolutionBackward0]
	140371954745504 -> 140371954747808
	140371954745504 [label=ReluBackward0]
	140371954746848 -> 140371954745504
	140371954746848 [label=CudnnBatchNormBackward0]
	140371954747184 -> 140371954746848
	140371954747184 [label=ConvolutionBackward0]
	140371954747040 -> 140371954747184
	140371954747040 [label=ReluBackward0]
	140371954746752 -> 140371954747040
	140371954746752 [label=CudnnBatchNormBackward0]
	140371954746656 -> 140371954746752
	140371954746656 [label=ConvolutionBackward0]
	140371954748336 -> 140371954746656
	140371954748336 [label=CudnnBatchNormBackward0]
	140371954746080 -> 140371954748336
	140371954746080 [label=ConvolutionBackward0]
	140371954745648 -> 140371954746080
	140371954745648 [label=ReluBackward0]
	140371954745744 -> 140371954745648
	140371954745744 [label=CudnnBatchNormBackward0]
	140371954745792 -> 140371954745744
	140371954745792 [label=ConvolutionBackward0]
	140371954747280 -> 140371954745792
	140371954747280 [label=ReluBackward0]
	140371954747232 -> 140371954747280
	140371954747232 [label=CudnnBatchNormBackward0]
	140371955195136 -> 140371954747232
	140371955195136 [label=ConvolutionBackward0]
	140371955195616 -> 140371955195136
	140371955195616 [label=AddBackward0]
	140371955195760 -> 140371955195616
	140371955195760 [label=CudnnBatchNormBackward0]
	140371955195184 -> 140371955195760
	140371955195184 [label=ConvolutionBackward0]
	140371955195424 -> 140371955195184
	140371955195424 [label=ReluBackward0]
	140371955194800 -> 140371955195424
	140371955194800 [label=CudnnBatchNormBackward0]
	140371955195040 -> 140371955194800
	140371955195040 [label=ConvolutionBackward0]
	140371955192112 -> 140371955195040
	140371955192112 [label=ReluBackward0]
	140371955193888 -> 140371955192112
	140371955193888 [label=CudnnBatchNormBackward0]
	140371955194176 -> 140371955193888
	140371955194176 [label=ConvolutionBackward0]
	140371955195856 -> 140371955194176
	140371955195856 [label=AddBackward0]
	140371955193792 -> 140371955195856
	140371955193792 [label=CudnnBatchNormBackward0]
	140371955193600 -> 140371955193792
	140371955193600 [label=ConvolutionBackward0]
	140371955193360 -> 140371955193600
	140371955193360 [label=ReluBackward0]
	140371955193168 -> 140371955193360
	140371955193168 [label=CudnnBatchNormBackward0]
	140371955193216 -> 140371955193168
	140371955193216 [label=ConvolutionBackward0]
	140371955192736 -> 140371955193216
	140371955192736 [label=ReluBackward0]
	140371955192448 -> 140371955192736
	140371955192448 [label=CudnnBatchNormBackward0]
	140371955192496 -> 140371955192448
	140371955192496 [label=ConvolutionBackward0]
	140371955193744 -> 140371955192496
	140371955193744 [label=CudnnBatchNormBackward0]
	140371955191968 -> 140371955193744
	140371955191968 [label=ConvolutionBackward0]
	140371955192544 -> 140371955191968
	140371955192544 [label=ReluBackward0]
	140371955194272 -> 140371955192544
	140371955194272 [label=CudnnBatchNormBackward0]
	140371955114912 -> 140371955194272
	140371955114912 [label=ConvolutionBackward0]
	140371955115776 -> 140371955114912
	140371955115776 [label=ReluBackward0]
	140371955115344 -> 140371955115776
	140371955115344 [label=CudnnBatchNormBackward0]
	140371955117552 -> 140371955115344
	140371955117552 [label=ConvolutionBackward0]
	140371955117696 -> 140371955117552
	140371955117696 [label=AddBackward0]
	140371955117216 -> 140371955117696
	140371955117216 [label=CudnnBatchNormBackward0]
	140371955116976 -> 140371955117216
	140371955116976 [label=ConvolutionBackward0]
	140371955116832 -> 140371955116976
	140371955116832 [label=ReluBackward0]
	140371955116304 -> 140371955116832
	140371955116304 [label=CudnnBatchNormBackward0]
	140371955116448 -> 140371955116304
	140371955116448 [label=ConvolutionBackward0]
	140371955115920 -> 140371955116448
	140371955115920 [label=ReluBackward0]
	140371955116016 -> 140371955115920
	140371955116016 [label=CudnnBatchNormBackward0]
	140371955115392 -> 140371955116016
	140371955115392 [label=ConvolutionBackward0]
	140371955117168 -> 140371955115392
	140371955117168 [label=AddBackward0]
	140371955115248 -> 140371955117168
	140371955115248 [label=CudnnBatchNormBackward0]
	140371955115200 -> 140371955115248
	140371955115200 [label=ConvolutionBackward0]
	140371955114816 -> 140371955115200
	140371955114816 [label=ReluBackward0]
	140371955114768 -> 140371955114816
	140371955114768 [label=CudnnBatchNormBackward0]
	140371955114672 -> 140371955114768
	140371955114672 [label=ConvolutionBackward0]
	140371955114336 -> 140371955114672
	140371955114336 [label=ReluBackward0]
	140371955117120 -> 140371955114336
	140371955117120 [label=CudnnBatchNormBackward0]
	140371955117072 -> 140371955117120
	140371955117072 [label=ConvolutionBackward0]
	140371312515824 -> 140371955117072
	140371312515824 [label=AddBackward0]
	140371312514528 -> 140371312515824
	140371312514528 [label=CudnnBatchNormBackward0]
	140371312517072 -> 140371312514528
	140371312517072 [label=ConvolutionBackward0]
	140371312516928 -> 140371312517072
	140371312516928 [label=ReluBackward0]
	140371312516592 -> 140371312516928
	140371312516592 [label=CudnnBatchNormBackward0]
	140371312516640 -> 140371312516592
	140371312516640 [label=ConvolutionBackward0]
	140371312515872 -> 140371312516640
	140371312515872 [label=ReluBackward0]
	140371312515968 -> 140371312515872
	140371312515968 [label=CudnnBatchNormBackward0]
	140371312516112 -> 140371312515968
	140371312516112 [label=ConvolutionBackward0]
	140371312515536 -> 140371312516112
	140371312515536 [label=ReluBackward0]
	140371312515632 -> 140371312515536
	140371312515632 [label=CudnnBatchNormBackward0]
	140371312515008 -> 140371312515632
	140371312515008 [label=ConvolutionBackward0]
	140371312515200 -> 140371312515008
	140372658989952 [label="module.conv1.weight
 (32, 3, 3, 3)" fillcolor=lightblue]
	140372658989952 -> 140371312515200
	140371312515200 [label=AccumulateGrad]
	140371312515680 -> 140371312515632
	140372658990032 [label="module.bn1.weight
 (32)" fillcolor=lightblue]
	140372658990032 -> 140371312515680
	140371312515680 [label=AccumulateGrad]
	140371312515344 -> 140371312515632
	140372658990192 [label="module.bn1.bias
 (32)" fillcolor=lightblue]
	140372658990192 -> 140371312515344
	140371312515344 [label=AccumulateGrad]
	140371312515776 -> 140371312516112
	140372658953888 [label="module.layers.0.conv1.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	140372658953888 -> 140371312515776
	140371312515776 [label=AccumulateGrad]
	140371312516160 -> 140371312515968
	140372658953568 [label="module.layers.0.bn1.weight
 (32)" fillcolor=lightblue]
	140372658953568 -> 140371312516160
	140371312516160 [label=AccumulateGrad]
	140371312516064 -> 140371312515968
	140372658953728 [label="module.layers.0.bn1.bias
 (32)" fillcolor=lightblue]
	140372658953728 -> 140371312516064
	140371312516064 [label=AccumulateGrad]
	140371312514048 -> 140371312516640
	140372658953168 [label="module.layers.0.conv2.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140372658953168 -> 140371312514048
	140371312514048 [label=AccumulateGrad]
	140371312513952 -> 140371312516592
	140372658953248 [label="module.layers.0.bn2.weight
 (32)" fillcolor=lightblue]
	140372658953248 -> 140371312513952
	140371312513952 [label=AccumulateGrad]
	140371312516544 -> 140371312516592
	140372658952928 [label="module.layers.0.bn2.bias
 (32)" fillcolor=lightblue]
	140372658952928 -> 140371312516544
	140371312516544 [label=AccumulateGrad]
	140371312516880 -> 140371312517072
	140372658952288 [label="module.layers.0.conv3.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140372658952288 -> 140371312516880
	140371312516880 [label=AccumulateGrad]
	140371312517024 -> 140371312514528
	140372658951968 [label="module.layers.0.bn3.weight
 (16)" fillcolor=lightblue]
	140372658951968 -> 140371312517024
	140371312517024 [label=AccumulateGrad]
	140371312516736 -> 140371312514528
	140372658952128 [label="module.layers.0.bn3.bias
 (16)" fillcolor=lightblue]
	140372658952128 -> 140371312516736
	140371312516736 [label=AccumulateGrad]
	140371312516256 -> 140371312515824
	140371312516256 [label=CudnnBatchNormBackward0]
	140371312516496 -> 140371312516256
	140371312516496 [label=ConvolutionBackward0]
	140371312515536 -> 140371312516496
	140371312515440 -> 140371312516496
	140372658951488 [label="module.layers.0.shortcut.0.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140372658951488 -> 140371312515440
	140371312515440 [label=AccumulateGrad]
	140371312516976 -> 140371312516256
	140372658951648 [label="module.layers.0.shortcut.1.weight
 (16)" fillcolor=lightblue]
	140372658951648 -> 140371312516976
	140371312516976 [label=AccumulateGrad]
	140371312516832 -> 140371312516256
	140372658951568 [label="module.layers.0.shortcut.1.bias
 (16)" fillcolor=lightblue]
	140372658951568 -> 140371312516832
	140371312516832 [label=AccumulateGrad]
	140371312514960 -> 140371955117072
	140372658951168 [label="module.layers.1.conv1.weight
 (112, 16, 1, 1)" fillcolor=lightblue]
	140372658951168 -> 140371312514960
	140371312514960 [label=AccumulateGrad]
	140371955114240 -> 140371955117120
	140372658951088 [label="module.layers.1.bn1.weight
 (112)" fillcolor=lightblue]
	140372658951088 -> 140371955114240
	140371955114240 [label=AccumulateGrad]
	140371955117024 -> 140371955117120
	140372658950768 [label="module.layers.1.bn1.bias
 (112)" fillcolor=lightblue]
	140372658950768 -> 140371955117024
	140371955117024 [label=AccumulateGrad]
	140371955114192 -> 140371955114672
	140372658952688 [label="module.layers.1.conv2.weight
 (112, 1, 3, 3)" fillcolor=lightblue]
	140372658952688 -> 140371955114192
	140371955114192 [label=AccumulateGrad]
	140371955114864 -> 140371955114768
	140372658952528 [label="module.layers.1.bn2.weight
 (112)" fillcolor=lightblue]
	140372658952528 -> 140371955114864
	140371955114864 [label=AccumulateGrad]
	140371955114144 -> 140371955114768
	140372658952848 [label="module.layers.1.bn2.bias
 (112)" fillcolor=lightblue]
	140372658952848 -> 140371955114144
	140371955114144 [label=AccumulateGrad]
	140371955114528 -> 140371955115200
	140372659030992 [label="module.layers.1.conv3.weight
 (24, 112, 1, 1)" fillcolor=lightblue]
	140372659030992 -> 140371955114528
	140371955114528 [label=AccumulateGrad]
	140371955115056 -> 140371955115248
	140372659030752 [label="module.layers.1.bn3.weight
 (24)" fillcolor=lightblue]
	140372659030752 -> 140371955115056
	140371955115056 [label=AccumulateGrad]
	140371955115296 -> 140371955115248
	140372659030672 [label="module.layers.1.bn3.bias
 (24)" fillcolor=lightblue]
	140372659030672 -> 140371955115296
	140371955115296 [label=AccumulateGrad]
	140371955114960 -> 140371955117168
	140371955114960 [label=CudnnBatchNormBackward0]
	140371955114432 -> 140371955114960
	140371955114432 [label=ConvolutionBackward0]
	140371312515824 -> 140371955114432
	140371955114288 -> 140371955114432
	140372659030272 [label="module.layers.1.shortcut.0.weight
 (24, 16, 1, 1)" fillcolor=lightblue]
	140372659030272 -> 140371955114288
	140371955114288 [label=AccumulateGrad]
	140371955115152 -> 140371955114960
	140372659030192 [label="module.layers.1.shortcut.1.weight
 (24)" fillcolor=lightblue]
	140372659030192 -> 140371955115152
	140371955115152 [label=AccumulateGrad]
	140371955115104 -> 140371955114960
	140372659030112 [label="module.layers.1.shortcut.1.bias
 (24)" fillcolor=lightblue]
	140372659030112 -> 140371955115104
	140371955115104 [label=AccumulateGrad]
	140371955115584 -> 140371955115392
	140372659029712 [label="module.layers.2.conv1.weight
 (168, 24, 1, 1)" fillcolor=lightblue]
	140372659029712 -> 140371955115584
	140371955115584 [label=AccumulateGrad]
	140371955116064 -> 140371955116016
	140372659029632 [label="module.layers.2.bn1.weight
 (168)" fillcolor=lightblue]
	140372659029632 -> 140371955116064
	140371955116064 [label=AccumulateGrad]
	140371955115728 -> 140371955116016
	140372659029552 [label="module.layers.2.bn1.bias
 (168)" fillcolor=lightblue]
	140372659029552 -> 140371955115728
	140371955115728 [label=AccumulateGrad]
	140371955116160 -> 140371955116448
	140372659029072 [label="module.layers.2.conv2.weight
 (168, 1, 3, 3)" fillcolor=lightblue]
	140372659029072 -> 140371955116160
	140371955116160 [label=AccumulateGrad]
	140371955116496 -> 140371955116304
	140372659029152 [label="module.layers.2.bn2.weight
 (168)" fillcolor=lightblue]
	140372659029152 -> 140371955116496
	140371955116496 [label=AccumulateGrad]
	140371955116256 -> 140371955116304
	140372659028992 [label="module.layers.2.bn2.bias
 (168)" fillcolor=lightblue]
	140372659028992 -> 140371955116256
	140371955116256 [label=AccumulateGrad]
	140371955116784 -> 140371955116976
	140372659028432 [label="module.layers.2.conv3.weight
 (24, 168, 1, 1)" fillcolor=lightblue]
	140372659028432 -> 140371955116784
	140371955116784 [label=AccumulateGrad]
	140371955116928 -> 140371955117216
	140372659028352 [label="module.layers.2.bn3.weight
 (24)" fillcolor=lightblue]
	140372659028352 -> 140371955116928
	140371955116928 [label=AccumulateGrad]
	140371955116640 -> 140371955117216
	140372659028272 [label="module.layers.2.bn3.bias
 (24)" fillcolor=lightblue]
	140372659028272 -> 140371955116640
	140371955116640 [label=AccumulateGrad]
	140371955117168 -> 140371955117696
	140371955117648 -> 140371955117552
	140372659028592 [label="module.layers.3.conv1.weight
 (168, 24, 1, 1)" fillcolor=lightblue]
	140372659028592 -> 140371955117648
	140371955117648 [label=AccumulateGrad]
	140371955117936 -> 140371955115344
	140372659030912 [label="module.layers.3.bn1.weight
 (168)" fillcolor=lightblue]
	140372659030912 -> 140371955117936
	140371955117936 [label=AccumulateGrad]
	140371955117840 -> 140371955115344
	140372659030832 [label="module.layers.3.bn1.bias
 (168)" fillcolor=lightblue]
	140372659030832 -> 140371955117840
	140371955117840 [label=AccumulateGrad]
	140371955117408 -> 140371955114912
	140372658996384 [label="module.layers.3.conv2.weight
 (168, 1, 3, 3)" fillcolor=lightblue]
	140372658996384 -> 140371955117408
	140371955117408 [label=AccumulateGrad]
	140371955116592 -> 140371955194272
	140372658996464 [label="module.layers.3.bn2.weight
 (168)" fillcolor=lightblue]
	140372658996464 -> 140371955116592
	140371955116592 [label=AccumulateGrad]
	140371955117264 -> 140371955194272
	140372658996144 [label="module.layers.3.bn2.bias
 (168)" fillcolor=lightblue]
	140372658996144 -> 140371955117264
	140371955117264 [label=AccumulateGrad]
	140371955194416 -> 140371955191968
	140372658995744 [label="module.layers.3.conv3.weight
 (32, 168, 1, 1)" fillcolor=lightblue]
	140372658995744 -> 140371955194416
	140371955194416 [label=AccumulateGrad]
	140371955191872 -> 140371955193744
	140372658995664 [label="module.layers.3.bn3.weight
 (32)" fillcolor=lightblue]
	140372658995664 -> 140371955191872
	140371955191872 [label=AccumulateGrad]
	140371955192352 -> 140371955193744
	140372658995584 [label="module.layers.3.bn3.bias
 (32)" fillcolor=lightblue]
	140372658995584 -> 140371955192352
	140371955192352 [label=AccumulateGrad]
	140371955192064 -> 140371955192496
	140372658996304 [label="module.layers.4.conv1.weight
 (224, 32, 1, 1)" fillcolor=lightblue]
	140372658996304 -> 140371955192064
	140371955192064 [label=AccumulateGrad]
	140371955192784 -> 140371955192448
	140372658996224 [label="module.layers.4.bn1.weight
 (224)" fillcolor=lightblue]
	140372658996224 -> 140371955192784
	140371955192784 [label=AccumulateGrad]
	140371955192400 -> 140371955192448
	140372658996784 [label="module.layers.4.bn1.bias
 (224)" fillcolor=lightblue]
	140372658996784 -> 140371955192400
	140371955192400 [label=AccumulateGrad]
	140371955192832 -> 140371955193216
	140371917367872 [label="module.layers.4.conv2.weight
 (224, 1, 3, 3)" fillcolor=lightblue]
	140371917367872 -> 140371955192832
	140371955192832 [label=AccumulateGrad]
	140371955193120 -> 140371955193168
	140371917367792 [label="module.layers.4.bn2.weight
 (224)" fillcolor=lightblue]
	140371917367792 -> 140371955193120
	140371955193120 [label=AccumulateGrad]
	140371955192880 -> 140371955193168
	140371917367952 [label="module.layers.4.bn2.bias
 (224)" fillcolor=lightblue]
	140371917367952 -> 140371955192880
	140371955192880 [label=AccumulateGrad]
	140371955193312 -> 140371955193600
	140371917368352 [label="module.layers.4.conv3.weight
 (32, 224, 1, 1)" fillcolor=lightblue]
	140371917368352 -> 140371955193312
	140371955193312 [label=AccumulateGrad]
	140371955193696 -> 140371955193792
	140371917368432 [label="module.layers.4.bn3.weight
 (32)" fillcolor=lightblue]
	140371917368432 -> 140371955193696
	140371955193696 [label=AccumulateGrad]
	140371955193552 -> 140371955193792
	140371917368512 [label="module.layers.4.bn3.bias
 (32)" fillcolor=lightblue]
	140371917368512 -> 140371955193552
	140371955193552 [label=AccumulateGrad]
	140371955193744 -> 140371955195856
	140371955193456 -> 140371955194176
	140371917368912 [label="module.layers.5.conv1.weight
 (224, 32, 1, 1)" fillcolor=lightblue]
	140371917368912 -> 140371955193456
	140371955193456 [label=AccumulateGrad]
	140371955194368 -> 140371955193888
	140371917368992 [label="module.layers.5.bn1.weight
 (224)" fillcolor=lightblue]
	140371917368992 -> 140371955194368
	140371955194368 [label=AccumulateGrad]
	140371955193984 -> 140371955193888
	140371917369072 [label="module.layers.5.bn1.bias
 (224)" fillcolor=lightblue]
	140371917369072 -> 140371955193984
	140371955193984 [label=AccumulateGrad]
	140371955194944 -> 140371955195040
	140371900260656 [label="module.layers.5.conv2.weight
 (224, 1, 3, 3)" fillcolor=lightblue]
	140371900260656 -> 140371955194944
	140371955194944 [label=AccumulateGrad]
	140371955195328 -> 140371955194800
	140371900260576 [label="module.layers.5.bn2.weight
 (224)" fillcolor=lightblue]
	140371900260576 -> 140371955195328
	140371955195328 [label=AccumulateGrad]
	140371955194848 -> 140371955194800
	140371900260736 [label="module.layers.5.bn2.bias
 (224)" fillcolor=lightblue]
	140371900260736 -> 140371955194848
	140371955194848 [label=AccumulateGrad]
	140371955195280 -> 140371955195184
	140371900261136 [label="module.layers.5.conv3.weight
 (32, 224, 1, 1)" fillcolor=lightblue]
	140371900261136 -> 140371955195280
	140371955195280 [label=AccumulateGrad]
	140371955193408 -> 140371955195760
	140371900261216 [label="module.layers.5.bn3.weight
 (32)" fillcolor=lightblue]
	140371900261216 -> 140371955193408
	140371955193408 [label=AccumulateGrad]
	140371955195808 -> 140371955195760
	140371900261296 [label="module.layers.5.bn3.bias
 (32)" fillcolor=lightblue]
	140371900261296 -> 140371955195808
	140371955195808 [label=AccumulateGrad]
	140371955195856 -> 140371955195616
	140371955194656 -> 140371955195136
	140371900261696 [label="module.layers.6.conv1.weight
 (224, 32, 1, 1)" fillcolor=lightblue]
	140371900261696 -> 140371955194656
	140371955194656 [label=AccumulateGrad]
	140371955193840 -> 140371954747232
	140371900261776 [label="module.layers.6.bn1.weight
 (224)" fillcolor=lightblue]
	140371900261776 -> 140371955193840
	140371955193840 [label=AccumulateGrad]
	140371955194512 -> 140371954747232
	140371900261856 [label="module.layers.6.bn1.bias
 (224)" fillcolor=lightblue]
	140371900261856 -> 140371955194512
	140371955194512 [label=AccumulateGrad]
	140371954747328 -> 140371954745792
	140371900262336 [label="module.layers.6.conv2.weight
 (224, 1, 3, 3)" fillcolor=lightblue]
	140371900262336 -> 140371954747328
	140371954747328 [label=AccumulateGrad]
	140371954745840 -> 140371954745744
	140371900262256 [label="module.layers.6.bn2.weight
 (224)" fillcolor=lightblue]
	140371900262256 -> 140371954745840
	140371954745840 [label=AccumulateGrad]
	140371954745696 -> 140371954745744
	140371900262416 [label="module.layers.6.bn2.bias
 (224)" fillcolor=lightblue]
	140371900262416 -> 140371954745696
	140371954745696 [label=AccumulateGrad]
	140371954746176 -> 140371954746080
	140371900262816 [label="module.layers.6.conv3.weight
 (64, 224, 1, 1)" fillcolor=lightblue]
	140371900262816 -> 140371954746176
	140371954746176 [label=AccumulateGrad]
	140371954746320 -> 140371954748336
	140371900262896 [label="module.layers.6.bn3.weight
 (64)" fillcolor=lightblue]
	140371900262896 -> 140371954746320
	140371954746320 [label=AccumulateGrad]
	140371954746608 -> 140371954748336
	140371900262976 [label="module.layers.6.bn3.bias
 (64)" fillcolor=lightblue]
	140371900262976 -> 140371954746608
	140371954746608 [label=AccumulateGrad]
	140371954746272 -> 140371954746656
	140371900263376 [label="module.layers.7.conv1.weight
 (448, 64, 1, 1)" fillcolor=lightblue]
	140371900263376 -> 140371954746272
	140371954746272 [label=AccumulateGrad]
	140371954746512 -> 140371954746752
	140371900263456 [label="module.layers.7.bn1.weight
 (448)" fillcolor=lightblue]
	140371900263456 -> 140371954746512
	140371954746512 [label=AccumulateGrad]
	140371954746416 -> 140371954746752
	140371900263536 [label="module.layers.7.bn1.bias
 (448)" fillcolor=lightblue]
	140371900263536 -> 140371954746416
	140371954746416 [label=AccumulateGrad]
	140371954746992 -> 140371954747184
	140371900264016 [label="module.layers.7.conv2.weight
 (448, 1, 3, 3)" fillcolor=lightblue]
	140371900264016 -> 140371954746992
	140371954746992 [label=AccumulateGrad]
	140371954747136 -> 140371954746848
	140371900263936 [label="module.layers.7.bn2.weight
 (448)" fillcolor=lightblue]
	140371900263936 -> 140371954747136
	140371954747136 [label=AccumulateGrad]
	140371954747376 -> 140371954746848
	140371900264096 [label="module.layers.7.bn2.bias
 (448)" fillcolor=lightblue]
	140371900264096 -> 140371954747376
	140371954747376 [label=AccumulateGrad]
	140371954747904 -> 140371954747808
	140371900354704 [label="module.layers.7.conv3.weight
 (64, 448, 1, 1)" fillcolor=lightblue]
	140371900354704 -> 140371954747904
	140371954747904 [label=AccumulateGrad]
	140371954748048 -> 140371954747760
	140371900354784 [label="module.layers.7.bn3.weight
 (64)" fillcolor=lightblue]
	140371900354784 -> 140371954748048
	140371954748048 [label=AccumulateGrad]
	140371954748000 -> 140371954747760
	140371900354864 [label="module.layers.7.bn3.bias
 (64)" fillcolor=lightblue]
	140371900354864 -> 140371954748000
	140371954748000 [label=AccumulateGrad]
	140371954748336 -> 140371954747664
	140371954748288 -> 140371954748432
	140371900355264 [label="module.layers.8.conv1.weight
 (448, 64, 1, 1)" fillcolor=lightblue]
	140371900355264 -> 140371954748288
	140371954748288 [label=AccumulateGrad]
	140371954748144 -> 140371954746368
	140371900355344 [label="module.layers.8.bn1.weight
 (448)" fillcolor=lightblue]
	140371900355344 -> 140371954748144
	140371954748144 [label=AccumulateGrad]
	140371954748720 -> 140371954746368
	140371900355424 [label="module.layers.8.bn1.bias
 (448)" fillcolor=lightblue]
	140371900355424 -> 140371954748720
	140371954748720 [label=AccumulateGrad]
	140371954748672 -> 140371954748576
	140371900355904 [label="module.layers.8.conv2.weight
 (448, 1, 3, 3)" fillcolor=lightblue]
	140371900355904 -> 140371954748672
	140371954748672 [label=AccumulateGrad]
	140371954747520 -> 140371954749200
	140371900355824 [label="module.layers.8.bn2.weight
 (448)" fillcolor=lightblue]
	140371900355824 -> 140371954747520
	140371954747520 [label=AccumulateGrad]
	140371954749248 -> 140371954749200
	140371900355984 [label="module.layers.8.bn2.bias
 (448)" fillcolor=lightblue]
	140371900355984 -> 140371954749248
	140371954749248 [label=AccumulateGrad]
	140371954749344 -> 140371954745936
	140371900356384 [label="module.layers.8.conv3.weight
 (64, 448, 1, 1)" fillcolor=lightblue]
	140371900356384 -> 140371954749344
	140371954749344 [label=AccumulateGrad]
	140371954747568 -> 140371954748960
	140371900356464 [label="module.layers.8.bn3.weight
 (64)" fillcolor=lightblue]
	140371900356464 -> 140371954747568
	140371954747568 [label=AccumulateGrad]
	140371954748096 -> 140371954748960
	140371900356544 [label="module.layers.8.bn3.bias
 (64)" fillcolor=lightblue]
	140371900356544 -> 140371954748096
	140371954748096 [label=AccumulateGrad]
	140371954747664 -> 140371954832768
	140371954749392 -> 140371954833632
	140371900356944 [label="module.layers.9.conv1.weight
 (448, 64, 1, 1)" fillcolor=lightblue]
	140371900356944 -> 140371954749392
	140371954749392 [label=AccumulateGrad]
	140371954832240 -> 140371954833680
	140371900357024 [label="module.layers.9.bn1.weight
 (448)" fillcolor=lightblue]
	140371900357024 -> 140371954832240
	140371954832240 [label=AccumulateGrad]
	140371954831616 -> 140371954833680
	140371900357104 [label="module.layers.9.bn1.bias
 (448)" fillcolor=lightblue]
	140371900357104 -> 140371954831616
	140371954831616 [label=AccumulateGrad]
	140371954831712 -> 140371954831760
	140371900357584 [label="module.layers.9.conv2.weight
 (448, 1, 3, 3)" fillcolor=lightblue]
	140371900357584 -> 140371954831712
	140371954831712 [label=AccumulateGrad]
	140371954831520 -> 140371954832000
	140371900357504 [label="module.layers.9.bn2.weight
 (448)" fillcolor=lightblue]
	140371900357504 -> 140371954831520
	140371954831520 [label=AccumulateGrad]
	140371954832096 -> 140371954832000
	140371900357664 [label="module.layers.9.bn2.bias
 (448)" fillcolor=lightblue]
	140371900357664 -> 140371954832096
	140371954832096 [label=AccumulateGrad]
	140371954832192 -> 140371954833392
	140371900358064 [label="module.layers.9.conv3.weight
 (64, 448, 1, 1)" fillcolor=lightblue]
	140371900358064 -> 140371954832192
	140371954832192 [label=AccumulateGrad]
	140371954832288 -> 140371954833488
	140371900358144 [label="module.layers.9.bn3.weight
 (64)" fillcolor=lightblue]
	140371900358144 -> 140371954832288
	140371954832288 [label=AccumulateGrad]
	140371954833440 -> 140371954833488
	140371900358224 [label="module.layers.9.bn3.bias
 (64)" fillcolor=lightblue]
	140371900358224 -> 140371954833440
	140371954833440 [label=AccumulateGrad]
	140371954832768 -> 140371954832816
	140371954832672 -> 140371954832624
	140371899928640 [label="module.layers.10.conv1.weight
 (448, 64, 1, 1)" fillcolor=lightblue]
	140371899928640 -> 140371954832672
	140371954832672 [label=AccumulateGrad]
	140371954833200 -> 140371954833152
	140371899928720 [label="module.layers.10.bn1.weight
 (448)" fillcolor=lightblue]
	140371899928720 -> 140371954833200
	140371954833200 [label=AccumulateGrad]
	140371954833104 -> 140371954833152
	140371899928800 [label="module.layers.10.bn1.bias
 (448)" fillcolor=lightblue]
	140371899928800 -> 140371954833104
	140371954833104 [label=AccumulateGrad]
	140371954833296 -> 140371954833536
	140371899929280 [label="module.layers.10.conv2.weight
 (448, 1, 3, 3)" fillcolor=lightblue]
	140371899929280 -> 140371954833296
	140371954833296 [label=AccumulateGrad]
	140371954832528 -> 140371954833824
	140371899929200 [label="module.layers.10.bn2.weight
 (448)" fillcolor=lightblue]
	140371899929200 -> 140371954832528
	140371954832528 [label=AccumulateGrad]
	140371954832480 -> 140371954833824
	140371899929360 [label="module.layers.10.bn2.bias
 (448)" fillcolor=lightblue]
	140371899929360 -> 140371954832480
	140371954832480 [label=AccumulateGrad]
	140371954834064 -> 140371954834352
	140371899929760 [label="module.layers.10.conv3.weight
 (96, 448, 1, 1)" fillcolor=lightblue]
	140371899929760 -> 140371954834064
	140371954834064 [label=AccumulateGrad]
	140371954834304 -> 140371954834544
	140371899929840 [label="module.layers.10.bn3.weight
 (96)" fillcolor=lightblue]
	140371899929840 -> 140371954834304
	140371954834304 [label=AccumulateGrad]
	140371954834112 -> 140371954834544
	140371899929920 [label="module.layers.10.bn3.bias
 (96)" fillcolor=lightblue]
	140371899929920 -> 140371954834112
	140371954834112 [label=AccumulateGrad]
	140371954834496 -> 140371954914928
	140371954834496 [label=CudnnBatchNormBackward0]
	140371954833008 -> 140371954834496
	140371954833008 [label=ConvolutionBackward0]
	140371954832816 -> 140371954833008
	140371954832864 -> 140371954833008
	140371899930320 [label="module.layers.10.shortcut.0.weight
 (96, 64, 1, 1)" fillcolor=lightblue]
	140371899930320 -> 140371954832864
	140371954832864 [label=AccumulateGrad]
	140371954834256 -> 140371954834496
	140371899930400 [label="module.layers.10.shortcut.1.weight
 (96)" fillcolor=lightblue]
	140371899930400 -> 140371954834256
	140371954834256 [label=AccumulateGrad]
	140371954834208 -> 140371954834496
	140371899930480 [label="module.layers.10.shortcut.1.bias
 (96)" fillcolor=lightblue]
	140371899930480 -> 140371954834208
	140371954834208 [label=AccumulateGrad]
	140371954834640 -> 140371954834448
	140371899930880 [label="module.layers.11.conv1.weight
 (672, 96, 1, 1)" fillcolor=lightblue]
	140371899930880 -> 140371954834640
	140371954834640 [label=AccumulateGrad]
	140371954831856 -> 140371954835024
	140371899930960 [label="module.layers.11.bn1.weight
 (672)" fillcolor=lightblue]
	140371899930960 -> 140371954831856
	140371954831856 [label=AccumulateGrad]
	140371954835072 -> 140371954835024
	140371899931040 [label="module.layers.11.bn1.bias
 (672)" fillcolor=lightblue]
	140371899931040 -> 140371954835072
	140371954835072 [label=AccumulateGrad]
	140371954835168 -> 140371954833920
	140371899931520 [label="module.layers.11.conv2.weight
 (672, 1, 3, 3)" fillcolor=lightblue]
	140371899931520 -> 140371954835168
	140371954835168 [label=AccumulateGrad]
	140371954835360 -> 140371954835408
	140371899931440 [label="module.layers.11.bn2.weight
 (672)" fillcolor=lightblue]
	140371899931440 -> 140371954835360
	140371954835360 [label=AccumulateGrad]
	140371954832960 -> 140371954835408
	140371899931600 [label="module.layers.11.bn2.bias
 (672)" fillcolor=lightblue]
	140371899931600 -> 140371954832960
	140371954832960 [label=AccumulateGrad]
	140371954834400 -> 140371954833968
	140371899932000 [label="module.layers.11.conv3.weight
 (96, 672, 1, 1)" fillcolor=lightblue]
	140371899932000 -> 140371954834400
	140371954834400 [label=AccumulateGrad]
	140371954833872 -> 140371954914832
	140371899932080 [label="module.layers.11.bn3.weight
 (96)" fillcolor=lightblue]
	140371899932080 -> 140371954833872
	140371954833872 [label=AccumulateGrad]
	140371954835216 -> 140371954914832
	140371899932160 [label="module.layers.11.bn3.bias
 (96)" fillcolor=lightblue]
	140371899932160 -> 140371954835216
	140371954835216 [label=AccumulateGrad]
	140371954914928 -> 140371954914736
	140371954914400 -> 140371954913344
	140371899932560 [label="module.layers.12.conv1.weight
 (672, 96, 1, 1)" fillcolor=lightblue]
	140371899932560 -> 140371954914400
	140371954914400 [label=AccumulateGrad]
	140371954913488 -> 140371954913440
	140371900014656 [label="module.layers.12.bn1.weight
 (672)" fillcolor=lightblue]
	140371900014656 -> 140371954913488
	140371954913488 [label=AccumulateGrad]
	140371954913728 -> 140371954913440
	140371900014736 [label="module.layers.12.bn1.bias
 (672)" fillcolor=lightblue]
	140371900014736 -> 140371954913728
	140371954913728 [label=AccumulateGrad]
	140371954913680 -> 140371954913584
	140371900015216 [label="module.layers.12.conv2.weight
 (672, 1, 3, 3)" fillcolor=lightblue]
	140371900015216 -> 140371954913680
	140371954913680 [label=AccumulateGrad]
	140371954914208 -> 140371954914160
	140371900015136 [label="module.layers.12.bn2.weight
 (672)" fillcolor=lightblue]
	140371900015136 -> 140371954914208
	140371954914208 [label=AccumulateGrad]
	140371954914112 -> 140371954914160
	140371900015296 [label="module.layers.12.bn2.bias
 (672)" fillcolor=lightblue]
	140371900015296 -> 140371954914112
	140371954914112 [label=AccumulateGrad]
	140371954914304 -> 140371954914592
	140371900015696 [label="module.layers.12.conv3.weight
 (96, 672, 1, 1)" fillcolor=lightblue]
	140371900015696 -> 140371954914304
	140371954914304 [label=AccumulateGrad]
	140371954914688 -> 140371954914784
	140371900015776 [label="module.layers.12.bn3.weight
 (96)" fillcolor=lightblue]
	140371900015776 -> 140371954914688
	140371954914688 [label=AccumulateGrad]
	140371954914544 -> 140371954914784
	140371900015856 [label="module.layers.12.bn3.bias
 (96)" fillcolor=lightblue]
	140371900015856 -> 140371954914544
	140371954914544 [label=AccumulateGrad]
	140371954914736 -> 140371954915024
	140371954914976 -> 140371954915456
	140371900016256 [label="module.layers.13.conv1.weight
 (672, 96, 1, 1)" fillcolor=lightblue]
	140371900016256 -> 140371954914976
	140371954914976 [label=AccumulateGrad]
	140371954915552 -> 140371954915408
	140371900016336 [label="module.layers.13.bn1.weight
 (672)" fillcolor=lightblue]
	140371900016336 -> 140371954915552
	140371954915552 [label=AccumulateGrad]
	140371954915600 -> 140371954915408
	140371900016416 [label="module.layers.13.bn1.bias
 (672)" fillcolor=lightblue]
	140371900016416 -> 140371954915600
	140371954915600 [label=AccumulateGrad]
	140371954915936 -> 140371954915840
	140371900016896 [label="module.layers.13.conv2.weight
 (672, 1, 3, 3)" fillcolor=lightblue]
	140371900016896 -> 140371954915936
	140371954915936 [label=AccumulateGrad]
	140371954916080 -> 140371954916032
	140371900016816 [label="module.layers.13.bn2.weight
 (672)" fillcolor=lightblue]
	140371900016816 -> 140371954916080
	140371954916080 [label=AccumulateGrad]
	140371954916368 -> 140371954916032
	140371900016976 [label="module.layers.13.bn2.bias
 (672)" fillcolor=lightblue]
	140371900016976 -> 140371954916368
	140371954916368 [label=AccumulateGrad]
	140371954916416 -> 140371954916464
	140371900017376 [label="module.layers.13.conv3.weight
 (160, 672, 1, 1)" fillcolor=lightblue]
	140371900017376 -> 140371954916416
	140371954916416 [label=AccumulateGrad]
	140371954916176 -> 140371954470976
	140371900017456 [label="module.layers.13.bn3.weight
 (160)" fillcolor=lightblue]
	140371900017456 -> 140371954916176
	140371954916176 [label=AccumulateGrad]
	140371954916848 -> 140371954470976
	140371900017536 [label="module.layers.13.bn3.bias
 (160)" fillcolor=lightblue]
	140371900017536 -> 140371954916848
	140371954916848 [label=AccumulateGrad]
	140371954916800 -> 140371954916944
	140371900017936 [label="module.layers.14.conv1.weight
 (1120, 160, 1, 1)" fillcolor=lightblue]
	140371900017936 -> 140371954916800
	140371954916800 [label=AccumulateGrad]
	140371954916896 -> 140371954916608
	140371900018016 [label="module.layers.14.bn1.weight
 (1120)" fillcolor=lightblue]
	140371900018016 -> 140371954916896
	140371954916896 [label=AccumulateGrad]
	140371954917232 -> 140371954916608
	140371900018096 [label="module.layers.14.bn1.bias
 (1120)" fillcolor=lightblue]
	140371900018096 -> 140371954917232
	140371954917232 [label=AccumulateGrad]
	140371954917280 -> 140371954917040
	140371900018576 [label="module.layers.14.conv2.weight
 (1120, 1, 3, 3)" fillcolor=lightblue]
	140371900018576 -> 140371954917280
	140371954917280 [label=AccumulateGrad]
	140371954915264 -> 140371954915696
	140371900018496 [label="module.layers.14.bn2.weight
 (1120)" fillcolor=lightblue]
	140371900018496 -> 140371954915264
	140371954915264 [label=AccumulateGrad]
	140371954915120 -> 140371954915696
	140371900112960 [label="module.layers.14.bn2.bias
 (1120)" fillcolor=lightblue]
	140371900112960 -> 140371954915120
	140371954915120 [label=AccumulateGrad]
	140371954916560 -> 140371954471888
	140371900113360 [label="module.layers.14.conv3.weight
 (160, 1120, 1, 1)" fillcolor=lightblue]
	140371900113360 -> 140371954916560
	140371954916560 [label=AccumulateGrad]
	140371954471024 -> 140371954471984
	140371900113440 [label="module.layers.14.bn3.weight
 (160)" fillcolor=lightblue]
	140371900113440 -> 140371954471024
	140371954471024 [label=AccumulateGrad]
	140371954471936 -> 140371954471984
	140371900113520 [label="module.layers.14.bn3.bias
 (160)" fillcolor=lightblue]
	140371900113520 -> 140371954471936
	140371954471936 [label=AccumulateGrad]
	140371954470976 -> 140371954473280
	140371954471264 -> 140371954471408
	140371900113920 [label="module.layers.15.conv1.weight
 (1120, 160, 1, 1)" fillcolor=lightblue]
	140371900113920 -> 140371954471264
	140371954471264 [label=AccumulateGrad]
	140371954471360 -> 140371954471072
	140371900114000 [label="module.layers.15.bn1.weight
 (1120)" fillcolor=lightblue]
	140371900114000 -> 140371954471360
	140371954471360 [label=AccumulateGrad]
	140371954471648 -> 140371954471072
	140371900114080 [label="module.layers.15.bn1.bias
 (1120)" fillcolor=lightblue]
	140371900114080 -> 140371954471648
	140371954471648 [label=AccumulateGrad]
	140371954471600 -> 140371954471504
	140371900114560 [label="module.layers.15.conv2.weight
 (1120, 1, 3, 3)" fillcolor=lightblue]
	140371900114560 -> 140371954471600
	140371954471600 [label=AccumulateGrad]
	140371954472080 -> 140371954472032
	140371900114480 [label="module.layers.15.bn2.weight
 (1120)" fillcolor=lightblue]
	140371900114480 -> 140371954472080
	140371954472080 [label=AccumulateGrad]
	140371954472560 -> 140371954472032
	140371900114640 [label="module.layers.15.bn2.bias
 (1120)" fillcolor=lightblue]
	140371900114640 -> 140371954472560
	140371954472560 [label=AccumulateGrad]
	140371954472608 -> 140371954472656
	140371900115040 [label="module.layers.15.conv3.weight
 (160, 1120, 1, 1)" fillcolor=lightblue]
	140371900115040 -> 140371954472608
	140371954472608 [label=AccumulateGrad]
	140371954472416 -> 140371954473088
	140371900115120 [label="module.layers.15.bn3.weight
 (160)" fillcolor=lightblue]
	140371900115120 -> 140371954472416
	140371954472416 [label=AccumulateGrad]
	140371954473184 -> 140371954473088
	140371900115200 [label="module.layers.15.bn3.bias
 (160)" fillcolor=lightblue]
	140371900115200 -> 140371954473184
	140371954473184 [label=AccumulateGrad]
	140371954473280 -> 140371954473376
	140371954473328 -> 140371954473040
	140371900115600 [label="module.layers.16.conv1.weight
 (1120, 160, 1, 1)" fillcolor=lightblue]
	140371900115600 -> 140371954473328
	140371954473328 [label=AccumulateGrad]
	140371954473808 -> 140371954472944
	140371900115680 [label="module.layers.16.bn1.weight
 (1120)" fillcolor=lightblue]
	140371900115680 -> 140371954473808
	140371954473808 [label=AccumulateGrad]
	140371954472848 -> 140371954472944
	140371900115520 [label="module.layers.16.bn1.bias
 (1120)" fillcolor=lightblue]
	140371900115520 -> 140371954472848
	140371954472848 [label=AccumulateGrad]
	140371954473424 -> 140371954474816
	140371900116240 [label="module.layers.16.conv2.weight
 (1120, 1, 3, 3)" fillcolor=lightblue]
	140371900116240 -> 140371954473424
	140371954473424 [label=AccumulateGrad]
	140371954474960 -> 140371954474864
	140371900116160 [label="module.layers.16.bn2.weight
 (1120)" fillcolor=lightblue]
	140371900116160 -> 140371954474960
	140371954474960 [label=AccumulateGrad]
	140371954472272 -> 140371954474864
	140371900116320 [label="module.layers.16.bn2.bias
 (1120)" fillcolor=lightblue]
	140371900116320 -> 140371954472272
	140371954472272 [label=AccumulateGrad]
	140371954474576 -> 140371954474528
	140371900116720 [label="module.layers.16.conv3.weight
 (320, 1120, 1, 1)" fillcolor=lightblue]
	140371900116720 -> 140371954474576
	140371954474576 [label=AccumulateGrad]
	140371954474432 -> 140371954473856
	140371900116800 [label="module.layers.16.bn3.weight
 (320)" fillcolor=lightblue]
	140371900116800 -> 140371954474432
	140371954474432 [label=AccumulateGrad]
	140371954474480 -> 140371954473856
	140371900116880 [label="module.layers.16.bn3.bias
 (320)" fillcolor=lightblue]
	140371900116880 -> 140371954474480
	140371954474480 [label=AccumulateGrad]
	140371954474144 -> 140371954473952
	140371954474144 [label=CudnnBatchNormBackward0]
	140371954472800 -> 140371954474144
	140371954472800 [label=ConvolutionBackward0]
	140371954473376 -> 140371954472800
	140371954474240 -> 140371954472800
	140371899679104 [label="module.layers.16.shortcut.0.weight
 (320, 160, 1, 1)" fillcolor=lightblue]
	140371899679104 -> 140371954474240
	140371954474240 [label=AccumulateGrad]
	140371954474624 -> 140371954474144
	140371899679184 [label="module.layers.16.shortcut.1.weight
 (320)" fillcolor=lightblue]
	140371899679184 -> 140371954474624
	140371954474624 [label=AccumulateGrad]
	140371954474384 -> 140371954474144
	140371899679264 [label="module.layers.16.shortcut.1.bias
 (320)" fillcolor=lightblue]
	140371899679264 -> 140371954474384
	140371954474384 [label=AccumulateGrad]
	140371954474096 -> 140371954473616
	140371899679664 [label="module.conv2.weight
 (1280, 320, 1, 1)" fillcolor=lightblue]
	140371899679664 -> 140371954474096
	140371954474096 [label=AccumulateGrad]
	140371954473664 -> 140371954473520
	140371899679744 [label="module.bn2.weight
 (1280)" fillcolor=lightblue]
	140371899679744 -> 140371954473664
	140371954473664 [label=AccumulateGrad]
	140371954473712 -> 140371954473520
	140371899679824 [label="module.bn2.bias
 (1280)" fillcolor=lightblue]
	140371899679824 -> 140371954473712
	140371954473712 [label=AccumulateGrad]
	140371954555440 -> 140371954555488
	140371954555440 [label=TBackward0]
	140371954553712 -> 140371954555440
	140371899680144 [label="module.linear.weight
 (10, 1280)" fillcolor=lightblue]
	140371899680144 -> 140371954553712
	140371954553712 [label=AccumulateGrad]
	140371954554912 -> 140371899680704
}
